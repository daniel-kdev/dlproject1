{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2cc39cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\oykwon\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\oykwon\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "## opencv 설치\n",
    "!pip install opencv-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3c5638-2a35-4787-8359-0f46a67a1dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 64, 64), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=2, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        acc = 0.0\n",
    "        \n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt) \n",
    "        \n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다（수치미분）.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_w = lambda w: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in (1, 2, 3):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "        \n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
    "            self.layers[key].W = self.params['W' + str(i+1)]\n",
    "            self.layers[key].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fafb668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(642, 1, 64, 64)\n",
      "train loss:2.6646584605391874\n",
      "=== epoch:1, train acc:0.6588785046728972, test acc:0.6895424836601307 ===\n",
      "train loss:-9.999999345966605e-08\n",
      "train loss:9.263305524666848\n",
      "train loss:1.3206593534738889\n",
      "train loss:2.0582434997365153\n",
      "train loss:2.0284567436347487\n",
      "train loss:1.15472107339392\n",
      "train loss:0.6946147605709923\n",
      "train loss:0.6381667660604872\n",
      "train loss:0.7259954967186769\n",
      "train loss:0.5659390520363929\n",
      "train loss:0.7116300574136943\n",
      "train loss:0.7063112604356512\n",
      "train loss:0.6756610752350924\n",
      "train loss:0.5059460077875917\n",
      "train loss:0.5855436996513539\n",
      "train loss:0.7428462629119542\n",
      "train loss:0.8238532561088159\n",
      "train loss:0.6031256545494551\n",
      "train loss:0.5057226091911236\n",
      "train loss:0.6112886664284022\n",
      "train loss:0.6818685013351867\n",
      "train loss:0.6509270721537458\n",
      "train loss:0.47788080520010023\n",
      "train loss:0.8031454938657665\n",
      "train loss:0.43789405499196904\n",
      "train loss:0.5021177975584551\n",
      "train loss:0.5305687627335147\n",
      "train loss:0.6750206802075167\n",
      "train loss:0.7386323472331153\n",
      "train loss:0.30507184292889555\n",
      "train loss:0.27610108767314945\n",
      "train loss:0.4797376974676967\n",
      "train loss:0.8075610108217794\n",
      "train loss:0.8718780251784809\n",
      "train loss:0.7789869994854126\n",
      "train loss:1.1328007050566398\n",
      "train loss:0.8888852449317376\n",
      "train loss:0.8778420099946203\n",
      "train loss:0.4104725717555091\n",
      "train loss:0.6032641528109716\n",
      "train loss:0.6803497474095099\n",
      "train loss:0.5710306247327035\n",
      "train loss:0.4947328933797975\n",
      "train loss:0.5748319869802563\n",
      "train loss:0.6663343759508285\n",
      "train loss:0.6022611372543805\n",
      "train loss:0.6974854227310263\n",
      "train loss:0.5809155578282316\n",
      "train loss:0.7644419397474649\n",
      "train loss:0.6958225911347433\n",
      "train loss:0.6553775368698865\n",
      "train loss:0.5976635218493764\n",
      "train loss:0.6852460382408361\n",
      "train loss:0.6753651003570613\n",
      "train loss:0.5043979831356085\n",
      "train loss:0.6597859702635549\n",
      "train loss:0.6567225620346024\n",
      "train loss:0.5716820705851289\n",
      "train loss:0.5852788339293068\n",
      "train loss:0.5737235326423106\n",
      "train loss:0.5715450829729958\n",
      "train loss:0.7713950045639187\n",
      "train loss:0.8018347357671516\n",
      "train loss:0.6619986101857878\n",
      "train loss:0.7676229439009861\n",
      "train loss:0.5641141977845969\n",
      "train loss:0.45239062370801825\n",
      "train loss:0.5662055615657248\n",
      "train loss:0.597427312623307\n",
      "train loss:0.37772483328061107\n",
      "train loss:1.1977330104627668\n",
      "train loss:0.6925953367222961\n",
      "train loss:0.5214321044219448\n",
      "train loss:0.6730633910133614\n",
      "train loss:0.5207858352681184\n",
      "train loss:0.5550542702971529\n",
      "train loss:0.5308098486335269\n",
      "train loss:0.636980344549683\n",
      "train loss:0.530405486403897\n",
      "train loss:0.5412740056284474\n",
      "train loss:0.3778679244434736\n",
      "train loss:0.5288944463090779\n",
      "train loss:0.5268085267327098\n",
      "train loss:0.8830546666373369\n",
      "train loss:0.5990541568058857\n",
      "train loss:0.2886972248867493\n",
      "train loss:0.5157490379526436\n",
      "train loss:0.8150635934383041\n",
      "train loss:0.7388679919813088\n",
      "train loss:0.2489627980938573\n",
      "train loss:0.28834719611489235\n",
      "train loss:0.21228099390990862\n",
      "train loss:0.6796325538770815\n",
      "train loss:0.7996662746426243\n",
      "train loss:0.9627702774037905\n",
      "train loss:0.2625596456015976\n",
      "train loss:0.5530650673215395\n",
      "train loss:0.43301755066771985\n",
      "train loss:0.8028443550827682\n",
      "train loss:0.7447762858954772\n",
      "train loss:0.49975015714974685\n",
      "train loss:0.7764225332511667\n",
      "train loss:0.5531042988282583\n",
      "train loss:0.7596293518501369\n",
      "train loss:0.306281159379779\n",
      "train loss:0.31136912033065867\n",
      "train loss:0.5204961334316669\n",
      "train loss:0.5167439194064458\n",
      "train loss:0.6139176029673811\n",
      "train loss:0.5148728357975789\n",
      "train loss:0.6674881667015884\n",
      "train loss:0.5523881189285241\n",
      "train loss:0.655965871871089\n",
      "train loss:0.7132653549897824\n",
      "train loss:0.9507562898531253\n",
      "train loss:0.681035077997042\n",
      "train loss:0.5266522103329188\n",
      "train loss:0.8200174289795754\n",
      "train loss:0.5297893276322947\n",
      "train loss:0.8135052582756064\n",
      "train loss:0.7761450928505149\n",
      "train loss:0.5342454410667228\n",
      "train loss:0.4219832866349832\n",
      "train loss:0.5409196538246384\n",
      "train loss:0.7523283076221761\n",
      "train loss:0.668917665986295\n",
      "train loss:0.6629943281700649\n",
      "train loss:0.5844693825361058\n",
      "train loss:0.9073729079390294\n",
      "train loss:0.5582242825086402\n",
      "train loss:0.6761957983197668\n",
      "train loss:0.5493401561586708\n",
      "train loss:0.5915613580002193\n",
      "train loss:0.7605978191858235\n",
      "train loss:0.6833864087259178\n",
      "train loss:0.6729739703614683\n",
      "train loss:0.5604418080680647\n",
      "train loss:0.5607148076659854\n",
      "train loss:0.5683934716194112\n",
      "train loss:0.6790800649342126\n",
      "train loss:0.8021686277439176\n",
      "train loss:0.4157240138246706\n",
      "train loss:0.42967460341950015\n",
      "train loss:0.7008096065938796\n",
      "train loss:0.8075062867124517\n",
      "train loss:0.7673270327438952\n",
      "train loss:0.5186922002836845\n",
      "train loss:0.4157366620191004\n",
      "train loss:0.5601123103976422\n",
      "train loss:0.8677438365072231\n",
      "train loss:0.5156178091230931\n",
      "train loss:0.827573305645639\n",
      "train loss:0.6944122511761457\n",
      "train loss:0.5115684766129086\n",
      "train loss:0.6754360643901055\n",
      "train loss:0.6594161216981764\n",
      "train loss:0.6804900001473397\n",
      "train loss:0.38945941250840066\n",
      "train loss:0.7800516495798071\n",
      "train loss:0.5804378763919769\n",
      "train loss:0.48595337351376894\n",
      "train loss:0.375117010264023\n",
      "train loss:0.5335119421359724\n",
      "train loss:0.55496462509812\n",
      "train loss:0.6999456994079704\n",
      "train loss:0.4749939713950438\n",
      "train loss:0.5866685303160496\n",
      "train loss:0.49723279040231716\n",
      "train loss:0.2840119514893994\n",
      "train loss:1.0634413764277912\n",
      "train loss:0.7459145256797042\n",
      "train loss:0.5137435900459939\n",
      "train loss:0.4857399042344177\n",
      "train loss:0.538604376825673\n",
      "train loss:0.5223473303156501\n",
      "train loss:0.5194014460667036\n",
      "train loss:0.7092408137125831\n",
      "train loss:0.6885851928115059\n",
      "train loss:0.9169940677655373\n",
      "train loss:0.29342882324578945\n",
      "train loss:0.6379570363816659\n",
      "train loss:0.5508356157172661\n",
      "train loss:0.9112723589849085\n",
      "train loss:0.5501265315796344\n",
      "train loss:0.5140072739618611\n",
      "train loss:0.8476951306684445\n",
      "train loss:0.7131700454664873\n",
      "train loss:0.6672389774758696\n",
      "train loss:0.5544009959230284\n",
      "train loss:0.5505276377642121\n",
      "train loss:0.6893407848106722\n",
      "train loss:0.47032888655968275\n",
      "train loss:0.541414912083788\n",
      "train loss:0.6924711040473992\n",
      "train loss:0.5578192988560785\n",
      "train loss:0.6512352603854769\n",
      "train loss:0.804820332117804\n",
      "train loss:0.7994373326088929\n",
      "train loss:0.44351054705590237\n",
      "train loss:0.5674111848667259\n",
      "train loss:0.43150186013774877\n",
      "train loss:0.5634338285363001\n",
      "train loss:0.9147784039551945\n",
      "train loss:0.6745095037003598\n",
      "train loss:0.5153646030237219\n",
      "train loss:0.6939051910924692\n",
      "train loss:0.635389223704725\n",
      "train loss:0.5357516756481737\n",
      "train loss:0.8288126129028036\n",
      "train loss:0.3789844123610539\n",
      "train loss:0.8789547530873902\n",
      "train loss:0.5363341140221992\n",
      "train loss:0.3981612882189743\n",
      "train loss:0.6257001145344245\n",
      "train loss:0.6884379670131416\n",
      "train loss:0.4849109050733171\n",
      "train loss:0.899444186581988\n",
      "train loss:0.6269824171620618\n",
      "train loss:0.5066286335467834\n",
      "train loss:0.5603182190655865\n",
      "train loss:0.866272977885034\n",
      "train loss:0.6826569087410724\n",
      "train loss:0.685880490442573\n",
      "train loss:0.9895386705222926\n",
      "train loss:0.42400209865054145\n",
      "train loss:0.6822583833171099\n",
      "train loss:0.5605263870159072\n",
      "train loss:0.5349990564799646\n",
      "train loss:0.5282890016306379\n",
      "train loss:0.779173595182545\n",
      "train loss:0.6742609026825003\n",
      "train loss:0.5605904220980659\n",
      "train loss:0.7968296187447701\n",
      "train loss:0.837574119925808\n",
      "train loss:0.6545383905186833\n",
      "train loss:0.67415549943931\n",
      "train loss:0.673353741122969\n",
      "train loss:0.7785206908959503\n",
      "train loss:0.5538723803006481\n",
      "train loss:0.7434634982818886\n",
      "train loss:0.756657060054263\n",
      "train loss:0.8300113996296771\n",
      "train loss:0.6131043705898354\n",
      "train loss:0.6089123050746752\n",
      "train loss:0.6130985679537473\n",
      "train loss:0.7086547514356377\n",
      "train loss:0.6148974284908433\n",
      "train loss:0.685419460657126\n",
      "train loss:0.570275115346296\n",
      "train loss:0.6083538967776899\n",
      "train loss:0.7018305277972052\n",
      "train loss:0.5790251576374066\n",
      "train loss:0.6211657306647438\n",
      "train loss:0.7518964997216849\n",
      "train loss:0.6575823584286078\n",
      "train loss:0.611878588027497\n",
      "train loss:0.652483666007622\n",
      "train loss:0.5025475131047022\n",
      "train loss:0.4980593208221201\n",
      "train loss:0.5655052105215794\n",
      "train loss:0.5001458086393769\n",
      "train loss:0.5637769812515425\n",
      "train loss:0.6889984111602642\n",
      "train loss:0.7543833082942755\n",
      "train loss:0.6880155925450675\n",
      "train loss:0.8946575901025569\n",
      "train loss:0.6746361948991422\n",
      "train loss:0.37338660324551165\n",
      "train loss:0.36642173129988437\n",
      "train loss:0.654459434611904\n",
      "train loss:0.6574408568335445\n",
      "train loss:0.5331783964020154\n",
      "train loss:0.6970851151145954\n",
      "train loss:0.5413280110450425\n",
      "train loss:0.6752294282925088\n",
      "train loss:0.5234329705303404\n",
      "train loss:0.4674128306703421\n",
      "train loss:0.5909143157282071\n",
      "train loss:1.1954826015509352\n",
      "train loss:0.9012919397591194\n",
      "train loss:0.6797151418906775\n",
      "train loss:0.6995855071036614\n",
      "train loss:0.5161586154734936\n",
      "train loss:0.7810797547744941\n",
      "train loss:0.7863253951359421\n",
      "train loss:0.6033713883173715\n",
      "train loss:0.6388968480492677\n",
      "train loss:0.627797465879688\n",
      "train loss:0.5207288234133565\n",
      "train loss:0.5843709961434541\n",
      "train loss:0.506215494147676\n",
      "train loss:0.5658518718911016\n",
      "train loss:0.6771706240752069\n",
      "train loss:0.6132864031595953\n",
      "train loss:0.5896393713694674\n",
      "train loss:0.6686708942146469\n",
      "train loss:0.4774210434167854\n",
      "train loss:0.7535927678194517\n",
      "train loss:0.7898576723286471\n",
      "train loss:0.7865880133265317\n",
      "train loss:0.5388771486075308\n",
      "train loss:0.5577653996836288\n",
      "train loss:0.5476119763439019\n",
      "train loss:0.3997774485871578\n",
      "train loss:0.7916448844235093\n",
      "train loss:0.44673151289125207\n",
      "train loss:0.838341791275349\n",
      "train loss:0.5203668330303609\n",
      "train loss:0.6876846257373198\n",
      "train loss:0.5581428524489357\n",
      "train loss:0.7096717471790812\n",
      "train loss:0.5325825619249147\n",
      "train loss:0.6628916558819482\n",
      "train loss:0.34899690978766756\n",
      "train loss:0.515875948556122\n",
      "train loss:0.6520682048483111\n",
      "train loss:0.7247704806955398\n",
      "train loss:0.5482567605814997\n",
      "train loss:0.692326660605247\n",
      "train loss:0.5052726507164592\n",
      "train loss:0.8904886440268951\n",
      "train loss:0.7142100145057301\n",
      "train loss:0.9067483982121483\n",
      "train loss:0.3682330424150831\n",
      "train loss:0.8214666326022625\n",
      "train loss:0.48041377352826853\n",
      "train loss:0.5170882314891451\n",
      "train loss:0.5110465387865877\n",
      "train loss:0.4677989186220401\n",
      "train loss:0.8751323315761736\n",
      "train loss:0.49923206794757735\n",
      "train loss:0.4529949266960326\n",
      "train loss:0.382802006863099\n",
      "train loss:0.8574045119262148\n",
      "train loss:0.9358121871478253\n",
      "train loss:0.727271771452112\n",
      "train loss:0.9539153289032878\n",
      "train loss:0.6396879537568327\n",
      "train loss:0.5780675398004721\n",
      "train loss:0.6880742520948724\n",
      "train loss:0.6699423297292204\n",
      "train loss:0.6886046991216168\n",
      "train loss:0.7646574318512394\n",
      "train loss:0.6828318915852284\n",
      "train loss:0.5959436096349778\n",
      "train loss:0.6316374834930247\n",
      "train loss:0.594438870974002\n",
      "train loss:0.5245395660491355\n",
      "train loss:0.6956651556263281\n",
      "train loss:0.5845737392204063\n",
      "train loss:0.5172661454097931\n",
      "train loss:0.8234499373449689\n",
      "train loss:0.7467720747889472\n",
      "train loss:0.8421372276462158\n",
      "train loss:0.6523664062574177\n",
      "train loss:0.5977520019485715\n",
      "train loss:0.5941724734798095\n",
      "train loss:0.6547517628262793\n",
      "train loss:0.6854532396146585\n",
      "train loss:0.7384716875918665\n",
      "train loss:0.6921965820308447\n",
      "train loss:0.5245629011128157\n",
      "train loss:0.7940761916821881\n",
      "train loss:0.7640128652630435\n",
      "train loss:0.7058061276485088\n",
      "train loss:0.5217875408940742\n",
      "train loss:0.5217578525601038\n",
      "train loss:0.9100100205746576\n",
      "train loss:0.5931676588362164\n",
      "train loss:0.6962016065042046\n",
      "train loss:0.5706221823388261\n",
      "train loss:0.738693317537457\n",
      "train loss:0.5919967386776512\n",
      "train loss:0.7365930434826138\n",
      "train loss:0.6764160734388728\n",
      "train loss:0.666927736151445\n",
      "train loss:0.578163236509147\n",
      "train loss:0.6026114024003058\n",
      "train loss:0.6559858745579693\n",
      "train loss:0.5556368825853294\n",
      "train loss:0.4742320044535459\n",
      "train loss:0.6845378586925153\n",
      "train loss:0.649636649620293\n",
      "train loss:0.4480484742488313\n",
      "train loss:0.591117800674382\n",
      "train loss:0.6667385862818977\n",
      "train loss:0.5456533523861683\n",
      "train loss:0.5332369702380488\n",
      "train loss:0.5598501436908594\n",
      "train loss:0.5059537217503485\n",
      "train loss:0.32392516175476305\n",
      "train loss:0.9172204531094283\n",
      "train loss:0.4830960284540424\n",
      "train loss:0.6726281019156197\n",
      "train loss:0.8980921775802161\n",
      "train loss:0.7386783183154686\n",
      "train loss:0.5390941916859627\n",
      "train loss:0.29794758461525206\n",
      "train loss:0.3108357315148053\n",
      "train loss:0.7420945246570094\n",
      "train loss:0.473104923955822\n",
      "train loss:0.6820011986368939\n",
      "train loss:0.5691520206026903\n",
      "train loss:0.5147910327194907\n",
      "train loss:1.0226039726077574\n",
      "train loss:0.5225503474970414\n",
      "train loss:1.1593038926942214\n",
      "train loss:0.30968195254353936\n",
      "train loss:0.33870176008979275\n",
      "train loss:0.34699960215142395\n",
      "train loss:0.4987363768525338\n",
      "train loss:0.7048994606982035\n",
      "train loss:0.7222937853641322\n",
      "train loss:0.4666906132519146\n",
      "train loss:0.6192302333066682\n",
      "train loss:0.694119504451872\n",
      "train loss:0.5632481144125497\n",
      "train loss:0.3249062683310037\n",
      "train loss:0.7909776235526904\n",
      "train loss:0.5152885843887767\n",
      "train loss:0.6808563219515943\n",
      "train loss:0.6744580375464138\n",
      "train loss:0.8555705512885499\n",
      "train loss:0.8356880123338988\n",
      "train loss:0.5733236520935051\n",
      "train loss:0.7013418909080832\n",
      "train loss:0.6581020553251211\n",
      "train loss:0.5520109301297704\n",
      "train loss:0.6521504974103136\n",
      "train loss:0.4283334976952422\n",
      "train loss:0.6280466510238363\n",
      "train loss:0.7004357443800732\n",
      "train loss:0.5961790292624807\n",
      "train loss:0.7488193922588106\n",
      "train loss:0.5501742318713773\n",
      "train loss:0.5258946106566948\n",
      "train loss:0.7921153230688617\n",
      "train loss:0.5618570708968276\n",
      "train loss:0.9029068657468468\n",
      "train loss:0.7744544060658755\n",
      "train loss:0.8626761523791104\n",
      "train loss:0.41685663785609606\n",
      "train loss:0.5687720701103729\n",
      "train loss:0.5875107797948813\n",
      "train loss:0.46308878224870575\n",
      "train loss:0.4789439626092252\n",
      "train loss:0.5117852144197488\n",
      "train loss:0.5740847591953092\n",
      "train loss:0.6660356151270843\n",
      "train loss:0.5504180168300487\n",
      "train loss:0.5551832906314077\n",
      "train loss:0.6971342849577632\n",
      "train loss:0.515730141652879\n",
      "train loss:0.33278421582324136\n",
      "train loss:0.38005847835058126\n",
      "train loss:0.30584421637716785\n",
      "train loss:0.9030145002941969\n",
      "train loss:0.8729079233317035\n",
      "train loss:0.7119711747204163\n",
      "train loss:0.5578806854392261\n",
      "train loss:0.43341144985535696\n",
      "train loss:0.5453361785438635\n",
      "train loss:0.2579510647640787\n",
      "train loss:0.7500733795981331\n",
      "train loss:0.4479327251078308\n",
      "train loss:0.5546977207887076\n",
      "train loss:0.5355544557284622\n",
      "train loss:0.5026098504810748\n",
      "train loss:0.4914464842285966\n",
      "train loss:0.6968689561653872\n",
      "train loss:0.22812416404850175\n",
      "train loss:0.6798433115873996\n",
      "train loss:0.20428604647996465\n",
      "train loss:0.47089372251672856\n",
      "train loss:0.9391748397598949\n",
      "train loss:0.7903038790000896\n",
      "train loss:0.49564343818609585\n",
      "train loss:1.0416323064399085\n",
      "train loss:0.728959668164627\n",
      "train loss:0.5113200286959673\n",
      "train loss:0.33424409407643857\n",
      "train loss:0.4903221974739897\n",
      "train loss:0.506586962263535\n",
      "train loss:0.6840149821396591\n",
      "train loss:0.6944149728798734\n",
      "train loss:0.6524215291693709\n",
      "train loss:0.3685188608031474\n",
      "train loss:0.7187647411020306\n",
      "train loss:0.34565319457064847\n",
      "train loss:0.5414005385383346\n",
      "train loss:0.5203644555157474\n",
      "train loss:0.7152161332246072\n",
      "train loss:0.6779619851097376\n",
      "train loss:0.489271157129043\n",
      "train loss:0.7104701877491897\n",
      "train loss:0.34325071331329393\n",
      "train loss:0.6984442233728347\n",
      "train loss:0.6298865045845922\n",
      "train loss:0.563018697906844\n",
      "train loss:0.3026750183347896\n",
      "train loss:0.8815896271570199\n",
      "train loss:0.7524988042137195\n",
      "train loss:0.6073209261376985\n",
      "train loss:0.4735895313590657\n",
      "train loss:0.5321809484741122\n",
      "train loss:0.35768322938292896\n",
      "train loss:0.6185124481739375\n",
      "train loss:0.835888318625375\n",
      "train loss:0.5299070249817557\n",
      "train loss:0.5253483820245444\n",
      "train loss:0.6680377859461173\n",
      "train loss:0.5562300441824422\n",
      "train loss:0.3041830511905682\n",
      "train loss:0.3417554879125503\n",
      "train loss:0.9509206278559972\n",
      "train loss:0.5415007093908212\n",
      "train loss:0.6023577002631496\n",
      "train loss:0.4950144451636368\n",
      "train loss:0.4680300730068051\n",
      "train loss:0.680644859148909\n",
      "train loss:0.45970524130802526\n",
      "train loss:0.957090640939797\n",
      "train loss:0.731451097497055\n",
      "train loss:0.6968979359453837\n",
      "train loss:0.730660552312502\n",
      "train loss:0.674484513266571\n",
      "train loss:0.6617681638502683\n",
      "train loss:0.5337345344374883\n",
      "train loss:0.4581492732107299\n",
      "train loss:0.6828988937423061\n",
      "train loss:0.3764604312474161\n",
      "train loss:0.46773608756402274\n",
      "train loss:0.3347847548851738\n",
      "train loss:0.4960933730319758\n",
      "train loss:0.48783453163428697\n",
      "train loss:0.6800284654661064\n",
      "train loss:0.7320119987421374\n",
      "train loss:0.7258398725920681\n",
      "train loss:0.6841622460665477\n",
      "train loss:0.5181866317580377\n",
      "train loss:0.3173126539520142\n",
      "train loss:0.555001184934923\n",
      "train loss:0.4778576821773233\n",
      "train loss:0.29958648957550676\n",
      "train loss:0.46160374265701565\n",
      "train loss:0.5745597272412284\n",
      "train loss:0.26044201697560554\n",
      "train loss:0.8072523328622392\n",
      "train loss:0.2194855273867032\n",
      "train loss:0.4680353407117483\n",
      "train loss:0.4770246934166561\n",
      "train loss:0.9022691730388466\n",
      "train loss:0.45804528527440047\n",
      "train loss:0.3986567778872302\n",
      "train loss:0.23823680669091366\n",
      "train loss:0.5418457149158113\n",
      "train loss:0.5746918013415481\n",
      "train loss:0.568970419037115\n",
      "train loss:1.0072184784904288\n",
      "train loss:0.49503210048222523\n",
      "train loss:0.46053042226660584\n",
      "train loss:0.26855810206291053\n",
      "train loss:0.7105715628508079\n",
      "train loss:1.087727473932181\n",
      "train loss:0.7203098515278114\n",
      "train loss:0.9102344942623771\n",
      "train loss:0.35366387682749856\n",
      "train loss:0.5404120575796846\n",
      "train loss:0.30860644312197144\n",
      "train loss:0.6791333110813158\n",
      "train loss:0.5398045036341067\n",
      "train loss:0.6597857243631318\n",
      "train loss:0.3972566981935328\n",
      "train loss:0.672906255520001\n",
      "train loss:0.7328152824100655\n",
      "train loss:0.6950125531293118\n",
      "train loss:0.5094261421290953\n",
      "train loss:0.5448527146858475\n",
      "train loss:0.5240701961138909\n",
      "train loss:0.840930882726776\n",
      "train loss:0.3722072899651273\n",
      "train loss:0.8060046195604798\n",
      "train loss:0.3959573389098808\n",
      "train loss:0.5042937178433304\n",
      "train loss:0.5720675178496804\n",
      "train loss:0.8469850024484975\n",
      "train loss:0.7197130864953145\n",
      "train loss:0.36274361723469223\n",
      "train loss:0.5143363812913728\n",
      "train loss:0.5198665583556179\n",
      "train loss:0.8921045165495622\n",
      "train loss:0.5064325178438082\n",
      "train loss:0.483986821228963\n",
      "train loss:0.321081159628107\n",
      "train loss:0.9268039245884951\n",
      "train loss:0.7873189868603754\n",
      "train loss:0.865763056646659\n",
      "train loss:0.34285664045087655\n",
      "train loss:0.7392060001361839\n",
      "train loss:0.37536572917149275\n",
      "train loss:0.38913208200898414\n",
      "train loss:0.5239402737823066\n",
      "train loss:0.5022936288764225\n",
      "train loss:0.34034430114349384\n",
      "train loss:0.614027007478571\n",
      "train loss:0.27363300168266236\n",
      "train loss:0.47844320588432454\n",
      "train loss:0.9276444908952126\n",
      "train loss:0.2769029422963015\n",
      "train loss:0.9773569784408099\n",
      "train loss:0.9811039561415565\n",
      "train loss:0.4962643734941743\n",
      "train loss:0.6743916128914952\n",
      "train loss:0.8745285172807037\n",
      "train loss:0.48092074181746625\n",
      "train loss:0.5442045251076542\n",
      "train loss:0.34358788474378793\n",
      "train loss:0.45507307706817307\n",
      "train loss:0.3584653334449057\n",
      "train loss:0.3037700025350721\n",
      "train loss:0.95580893999806\n",
      "train loss:0.6347682969389468\n",
      "train loss:0.5148403841626915\n",
      "train loss:1.103220053762855\n",
      "train loss:0.9298565475070413\n",
      "train loss:0.7141780971130408\n",
      "train loss:0.5102497660554006\n",
      "train loss:0.8454320133589908\n",
      "train loss:0.5375264163126504\n",
      "train loss:0.6866418965643495\n",
      "train loss:0.6518088572147224\n",
      "train loss:0.579683135718706\n",
      "train loss:0.6689243998156325\n",
      "train loss:0.6930181845756471\n",
      "train loss:0.6832304397527197\n",
      "train loss:0.48492490201754956\n",
      "train loss:0.6521634185041247\n",
      "train loss:0.6527386039035303\n",
      "train loss:0.5270628725335316\n",
      "train loss:0.5644130177034213\n",
      "train loss:0.39976977349721443\n",
      "train loss:0.687871395230957\n",
      "train loss:0.5580291338998702\n",
      "train loss:0.7863790834666087\n",
      "train loss:0.8301789682075214\n",
      "train loss:0.7600427604224406\n",
      "train loss:0.5636893771375993\n",
      "train loss:0.6943289365375167\n",
      "train loss:0.4253652419713775\n",
      "train loss:0.5475310309146268\n",
      "train loss:0.7885766813362506\n",
      "train loss:0.5698757324724869\n",
      "train loss:0.6033794252357622\n",
      "train loss:0.44841130263419365\n",
      "train loss:0.41286849802892417\n",
      "train loss:0.9022744298119448\n",
      "train loss:0.6608213976116545\n",
      "train loss:0.7644199685125311\n",
      "train loss:0.5239863289991195\n",
      "train loss:0.6272911045244871\n",
      "train loss:0.5847523410604535\n",
      "train loss:0.543602006573559\n",
      "train loss:0.6918776010393721\n",
      "train loss:0.4986971265926948\n",
      "train loss:0.6797543550549278\n",
      "train loss:0.512291589708421\n",
      "train loss:0.7630154570587891\n",
      "train loss:0.389740757348338\n",
      "train loss:0.5142409267308915\n",
      "train loss:1.0342471100306618\n",
      "train loss:0.8643425735828663\n",
      "train loss:0.8312947352386463\n",
      "train loss:0.5683918843957783\n",
      "train loss:0.5496743088024957\n",
      "train loss:0.7976618926506707\n",
      "train loss:0.6822607170094814\n",
      "train loss:0.7341956937544882\n",
      "train loss:0.43239028706664817\n",
      "train loss:0.7988143307031306\n",
      "train loss:0.8809263598729675\n",
      "train loss:0.4635531699329164\n",
      "train loss:0.6732247299995329\n",
      "train loss:0.8029720164571348\n",
      "train loss:0.5665043659489106\n",
      "train loss:0.5652721986161563\n",
      "train loss:0.5575925384840426\n",
      "train loss:0.5727403114439384\n",
      "train loss:0.5660091635554853\n",
      "train loss:0.6819330165489215\n",
      "train loss:0.7239705036849042\n",
      "train loss:0.5774782076829721\n",
      "train loss:0.5530345742200065\n",
      "train loss:0.6571099494086091\n",
      "train loss:0.6499014976980615\n",
      "train loss:0.4395828684310062\n",
      "train loss:0.6306073543728056\n",
      "train loss:0.6670496930959474\n",
      "train loss:0.6723667690172636\n",
      "train loss:0.3664919942750985\n",
      "train loss:0.5371425465868456\n",
      "train loss:0.5058007707098667\n",
      "train loss:0.48685941992823967\n",
      "train loss:0.5068614898946222\n",
      "train loss:0.7241806888628317\n",
      "train loss:1.0979551240783687\n",
      "train loss:0.8920554165097121\n",
      "train loss:0.6830659143494544\n",
      "train loss:0.37057204096147434\n",
      "train loss:0.8939274808431549\n",
      "train loss:1.1200996614545546\n",
      "train loss:0.5669056279915138\n",
      "train loss:0.3787484481766031\n",
      "train loss:0.6761709413533087\n",
      "train loss:0.44390377452474805\n",
      "train loss:0.5454457921703103\n",
      "train loss:0.6436050602305269\n",
      "train loss:0.8819173950160426\n",
      "train loss:0.5840114717121616\n",
      "train loss:0.6709250547566491\n",
      "train loss:0.5772483959440989\n",
      "train loss:0.6103841799896302\n",
      "train loss:0.44597373512272676\n",
      "train loss:0.5294650467871935\n",
      "train loss:0.42496337253838534\n",
      "train loss:0.5739581215066788\n",
      "train loss:0.5536715267659347\n",
      "train loss:0.6944955777892855\n",
      "train loss:0.9349892653487057\n",
      "train loss:0.6677456098483044\n",
      "train loss:0.4166719385111808\n",
      "train loss:0.8122354097253212\n",
      "train loss:0.506661654457311\n",
      "train loss:0.5091932777374689\n",
      "train loss:0.5244441394199522\n",
      "train loss:0.8496756165541285\n",
      "train loss:0.293632749399885\n",
      "train loss:0.6322229151631678\n",
      "train loss:0.7380882476686079\n",
      "train loss:0.7408826752829284\n",
      "train loss:0.7028857311240488\n",
      "train loss:0.5382126694862318\n",
      "train loss:0.45372527156684245\n",
      "train loss:0.7383325480183599\n",
      "train loss:0.4970607114160982\n",
      "train loss:0.8701602333760373\n",
      "train loss:0.6246193153425426\n",
      "train loss:0.5675367421438496\n",
      "train loss:0.6813713803404821\n",
      "train loss:0.6449719056797307\n",
      "train loss:0.66102375348033\n",
      "train loss:0.6700640039575652\n",
      "train loss:0.9483089641614763\n",
      "train loss:0.5726146357346493\n",
      "train loss:0.6890351251800562\n",
      "train loss:0.8788883210639045\n",
      "train loss:0.5440309261993849\n",
      "train loss:0.6076169212669873\n",
      "train loss:0.5979815841735961\n",
      "train loss:0.7866165313671696\n",
      "train loss:0.6988299523946886\n",
      "train loss:0.6877897280572985\n",
      "train loss:0.5911290357771298\n",
      "train loss:0.6467568711544378\n",
      "train loss:0.6739724441865345\n",
      "train loss:0.5710764168422477\n",
      "train loss:0.589589438127099\n",
      "train loss:0.757648427489685\n",
      "train loss:0.5968390386140913\n",
      "train loss:0.6798467816866445\n",
      "train loss:0.475342678155627\n",
      "train loss:0.6850328936139141\n",
      "train loss:0.6787105786110001\n",
      "train loss:0.66717142848772\n",
      "train loss:0.5555195714358273\n",
      "train loss:0.5835364240774666\n",
      "train loss:0.5623875170564585\n",
      "train loss:0.5928997234962463\n",
      "train loss:0.6807607074265467\n",
      "train loss:0.6453549273793898\n",
      "train loss:0.5524947425659543\n",
      "train loss:0.59873578733517\n",
      "train loss:0.7041985587766322\n",
      "train loss:0.5146135668737126\n",
      "train loss:0.6391926440624526\n",
      "train loss:0.8834065023983477\n",
      "train loss:0.6163340178089636\n",
      "train loss:0.5178029947601771\n",
      "train loss:0.9072097039867085\n",
      "train loss:0.5258398008544998\n",
      "train loss:0.39580605158247034\n",
      "train loss:0.6762916326998785\n",
      "train loss:0.9460662156917478\n",
      "train loss:0.537265277385125\n",
      "train loss:0.425605957024637\n",
      "train loss:0.5676611519543588\n",
      "train loss:0.39908099461009494\n",
      "train loss:0.5111562471674298\n",
      "train loss:0.6657760480214681\n",
      "train loss:0.851640160768223\n",
      "train loss:0.3793895035367844\n",
      "train loss:0.8283995688053801\n",
      "train loss:0.6906989347390933\n",
      "train loss:0.5400409207812633\n",
      "train loss:0.7071020419579022\n",
      "train loss:0.5684046831977521\n",
      "train loss:0.4956360724742181\n",
      "train loss:0.7582347865221739\n",
      "train loss:0.8073703165850603\n",
      "train loss:0.5420962815022504\n",
      "train loss:0.46857196181833916\n",
      "train loss:0.49538332440273924\n",
      "train loss:0.3966139176941686\n",
      "train loss:0.6900526449071499\n",
      "train loss:0.5046752804677046\n",
      "train loss:0.5449420253957679\n",
      "train loss:0.9230241810099823\n",
      "train loss:0.6110866957205169\n",
      "train loss:0.8004909076612842\n",
      "train loss:0.6405773319435516\n",
      "train loss:0.5596425041697289\n",
      "train loss:0.5720621818203488\n",
      "train loss:0.4886630905995236\n",
      "train loss:0.27199075044362575\n",
      "train loss:0.832199736829266\n",
      "train loss:0.9519652450870961\n",
      "train loss:0.7454539149170882\n",
      "train loss:0.5774550554434121\n",
      "train loss:0.42509223829359977\n",
      "train loss:0.6682194775949772\n",
      "train loss:0.49796956624196326\n",
      "train loss:0.46769549856833137\n",
      "train loss:0.577237170705088\n",
      "train loss:0.5469924517174174\n",
      "train loss:0.560241592387407\n",
      "train loss:0.7233893574039467\n",
      "train loss:0.801260132949657\n",
      "train loss:0.6861518419175306\n",
      "train loss:0.5952656206564038\n",
      "train loss:0.6300910353406947\n",
      "train loss:0.5582499363369344\n",
      "train loss:0.5803912847214631\n",
      "train loss:0.7309763592867997\n",
      "train loss:0.6486616898446368\n",
      "train loss:0.39689694764552974\n",
      "train loss:0.8133928573900956\n",
      "train loss:0.7941975185701902\n",
      "train loss:0.7156253993767524\n",
      "train loss:0.9730173849362321\n",
      "train loss:0.7472986392409796\n",
      "train loss:0.6805028759454305\n",
      "train loss:0.5634990312661103\n",
      "train loss:0.6420847536851433\n",
      "train loss:0.5548834142119359\n",
      "train loss:0.4316208861914544\n",
      "train loss:0.6484023649362449\n",
      "train loss:0.7187154185020752\n",
      "train loss:0.5750304002892894\n",
      "train loss:0.47984979291437646\n",
      "train loss:0.6767551033357635\n",
      "train loss:0.5442230720945325\n",
      "train loss:0.4830540008376845\n",
      "train loss:0.6590881860056287\n",
      "train loss:0.4727037286339333\n",
      "train loss:0.6572730600340095\n",
      "train loss:0.43164386792622267\n",
      "train loss:0.5601342973513154\n",
      "train loss:0.5095353564247638\n",
      "train loss:0.31019513972176477\n",
      "train loss:0.8291018824515511\n",
      "train loss:0.9132078267796914\n",
      "train loss:0.5361885530081528\n",
      "train loss:0.742927725366423\n",
      "train loss:0.6718462478161269\n",
      "train loss:0.544134239593635\n",
      "train loss:0.565369264317728\n",
      "train loss:0.5162417857360737\n",
      "train loss:0.6679103110653536\n",
      "train loss:1.1527416952678855\n",
      "train loss:0.8303905300570339\n",
      "train loss:0.8609596159050625\n",
      "train loss:0.6327811941771522\n",
      "train loss:0.6146390577314723\n",
      "train loss:0.5044946187911977\n",
      "train loss:0.5054640674264276\n",
      "train loss:0.5689770331776136\n",
      "train loss:0.5805822895094155\n",
      "train loss:0.5506171071971029\n",
      "train loss:0.5620498539831854\n",
      "train loss:0.5569224321627104\n",
      "train loss:0.9122603813315088\n",
      "train loss:0.5424079092241316\n",
      "train loss:0.5802376676074905\n",
      "train loss:0.6686915595422251\n",
      "train loss:0.6489291208678171\n",
      "train loss:0.4267842426469207\n",
      "train loss:0.5329781849495636\n",
      "train loss:0.5464269948714834\n",
      "train loss:0.5345892881161861\n",
      "train loss:0.720391837950148\n",
      "train loss:0.6737672317995573\n",
      "train loss:0.8463841650593474\n",
      "train loss:0.5837220736732347\n",
      "train loss:0.41036528142988155\n",
      "train loss:0.5167059644262715\n",
      "train loss:0.771100440205975\n",
      "train loss:0.6883867921502395\n",
      "train loss:0.3922269948788038\n",
      "train loss:0.6611664233286367\n",
      "train loss:0.3623986291800413\n",
      "train loss:0.5241234631161348\n",
      "train loss:0.521935164908348\n",
      "train loss:0.5152889487340491\n",
      "train loss:0.293816537762597\n",
      "train loss:0.5189107376178769\n",
      "train loss:0.4559672761020062\n",
      "train loss:0.5024903010364764\n",
      "train loss:0.15529928234685697\n",
      "train loss:0.631132782245212\n",
      "train loss:0.1977737613463096\n",
      "train loss:0.7751917720657855\n",
      "train loss:0.8124935121045107\n",
      "train loss:0.4265272391345607\n",
      "train loss:1.1337525143590081\n",
      "train loss:0.745801885122465\n",
      "train loss:0.5507776555114119\n",
      "train loss:0.5589688873216965\n",
      "train loss:0.48642441740260667\n",
      "train loss:0.20841475656430455\n",
      "train loss:0.642661075053825\n",
      "train loss:0.2883045624731986\n",
      "train loss:0.7987370190601772\n",
      "train loss:0.5709266709358121\n",
      "train loss:0.9358091958508222\n",
      "train loss:0.5410672155514441\n",
      "train loss:0.7284151285225452\n",
      "train loss:0.49375141214723844\n",
      "train loss:1.2160524693649766\n",
      "train loss:0.8704204510847889\n",
      "train loss:0.6632697721018037\n",
      "train loss:0.7021745921272272\n",
      "train loss:0.5590005129645289\n",
      "train loss:0.5572857513012587\n",
      "train loss:0.7628709386792808\n",
      "train loss:0.7974233568135825\n",
      "train loss:0.4870230148422207\n",
      "train loss:0.5928096868957595\n",
      "train loss:0.5738176449035686\n",
      "train loss:0.5724809537611428\n",
      "train loss:0.5847484071912026\n",
      "train loss:0.5772060268100181\n",
      "train loss:0.48525450759934785\n",
      "train loss:0.5589341190291514\n",
      "train loss:0.5477976735186637\n",
      "train loss:0.6781357322003289\n",
      "train loss:0.4400073111723618\n",
      "train loss:0.9151330668491642\n",
      "train loss:0.46408122054166706\n",
      "train loss:0.6617339680420862\n",
      "train loss:0.41560768541031595\n",
      "train loss:0.6422311676271808\n",
      "train loss:0.8209050864774815\n",
      "train loss:0.6929107221368206\n",
      "train loss:0.4998197844036305\n",
      "train loss:0.8033503269213021\n",
      "train loss:0.546407346069249\n",
      "train loss:0.5114409289822707\n",
      "train loss:0.5254467351820461\n",
      "train loss:0.48948053019349214\n",
      "train loss:0.5153864735056471\n",
      "train loss:0.520895347624043\n",
      "train loss:0.519035121820879\n",
      "train loss:0.8018439800024986\n",
      "train loss:0.7402390318401402\n",
      "train loss:0.33928416420967283\n",
      "train loss:0.4857784944522613\n",
      "train loss:0.99410845373346\n",
      "train loss:0.28985997099136884\n",
      "train loss:0.30223458734561814\n",
      "train loss:0.526182815470169\n",
      "train loss:0.5795352915355376\n",
      "train loss:0.2585166905476056\n",
      "train loss:0.2580834980486143\n",
      "train loss:0.5678863059529432\n",
      "train loss:0.6868751716463241\n",
      "train loss:0.46284393966783793\n",
      "train loss:0.829615349403855\n",
      "train loss:0.8160179300631318\n",
      "train loss:0.7655030498707107\n",
      "train loss:0.5081589913811231\n",
      "train loss:0.5871270581253049\n",
      "train loss:0.26524301562169\n",
      "train loss:1.0698892719580892\n",
      "train loss:0.9294333443120694\n",
      "train loss:0.3255155172505207\n",
      "train loss:0.3184540245766031\n",
      "train loss:0.6874514536077786\n",
      "train loss:0.6876666081016366\n",
      "train loss:0.34560822334433394\n",
      "train loss:1.054291129828675\n",
      "train loss:0.3628115870916646\n",
      "train loss:0.8572891280460595\n",
      "train loss:0.7313271266069236\n",
      "train loss:0.5056031509201595\n",
      "train loss:0.6987830742892748\n",
      "train loss:0.7354488222465758\n",
      "train loss:0.5465193823164691\n",
      "train loss:0.5390968312013771\n",
      "train loss:0.8401364623937507\n",
      "train loss:0.5403294547964451\n",
      "train loss:0.5552950083871633\n",
      "train loss:0.8126906958442582\n",
      "train loss:0.6971099066091828\n",
      "train loss:0.5860209836119579\n",
      "train loss:0.4598423977584035\n",
      "train loss:0.6538275468392625\n",
      "train loss:0.6808693249395477\n",
      "train loss:0.7053232218479358\n",
      "train loss:0.6351486014504824\n",
      "train loss:0.4364638587672749\n",
      "train loss:0.5659392279737961\n",
      "train loss:0.39280311147100155\n",
      "train loss:0.6941959701655789\n",
      "train loss:0.6712143085784282\n",
      "train loss:0.5076536203424277\n",
      "train loss:0.5361291472870345\n",
      "train loss:0.4220462801903445\n",
      "train loss:0.9849152302669466\n",
      "train loss:0.6904737195937283\n",
      "train loss:0.5370120392306924\n",
      "train loss:0.6684369250092833\n",
      "train loss:0.570302772178464\n",
      "train loss:0.5899373973499136\n",
      "train loss:0.33671087639825725\n",
      "train loss:0.525888765196836\n",
      "train loss:0.6816267185046497\n",
      "train loss:0.6762065219897142\n",
      "train loss:0.6955339141444163\n",
      "train loss:0.6993081760676899\n",
      "train loss:0.5218778142336629\n",
      "train loss:0.741864413798383\n",
      "train loss:0.7735717609280871\n",
      "train loss:0.49159571732371077\n",
      "train loss:0.553604825186491\n",
      "train loss:0.8813064005658487\n",
      "train loss:0.3773230704848808\n",
      "train loss:0.5229820403937638\n",
      "train loss:0.9660573908390703\n",
      "train loss:0.7054953006361188\n",
      "train loss:0.8760701906963195\n",
      "train loss:0.7227382011551462\n",
      "train loss:0.8577632708664196\n",
      "train loss:0.7982671815780114\n",
      "train loss:0.6647565609966202\n",
      "train loss:0.5764189705315337\n",
      "train loss:0.6792671886517827\n",
      "train loss:0.5720573420067528\n",
      "train loss:0.49434724635257454\n",
      "train loss:0.4872388256561787\n",
      "train loss:0.7881696866100512\n",
      "train loss:0.6647971288858485\n",
      "train loss:0.5897372043166863\n",
      "train loss:0.5764341325521147\n",
      "train loss:0.5832176024722606\n",
      "train loss:0.4838989222363064\n",
      "train loss:0.5460154483837345\n",
      "train loss:0.5632303213657526\n",
      "train loss:0.6737735836036125\n",
      "train loss:0.6705699213615486\n",
      "train loss:0.5681206774287734\n",
      "train loss:0.6884638839426269\n",
      "train loss:0.568160400904338\n",
      "train loss:0.6571621126535195\n",
      "train loss:0.5259837777425267\n",
      "train loss:0.5518290737743025\n",
      "train loss:0.5130479868425825\n",
      "train loss:0.3837205729197259\n",
      "train loss:0.6800785290747703\n",
      "train loss:0.6602403485239048\n",
      "train loss:0.5326903732110793\n",
      "train loss:0.536558561973868\n",
      "train loss:0.4840377778040934\n",
      "train loss:0.5129473350760152\n",
      "train loss:0.3090390456534962\n",
      "train loss:0.5179227107959675\n",
      "train loss:0.7494048559367783\n",
      "train loss:0.6733678579043331\n",
      "train loss:0.7265251791561154\n",
      "train loss:0.4995819419973314\n",
      "train loss:0.7091813683425373\n",
      "train loss:0.5154010584286904\n",
      "train loss:0.4784360987730648\n",
      "train loss:0.756103193844034\n",
      "train loss:0.7695086437965053\n",
      "train loss:0.28739310396622575\n",
      "train loss:1.073304718848345\n",
      "train loss:1.1671140704268264\n",
      "train loss:0.7053055680522794\n",
      "train loss:0.6985885145691543\n",
      "train loss:0.3736067877717497\n",
      "train loss:0.5514254099094781\n",
      "train loss:0.38702758311246865\n",
      "train loss:0.6979830698314407\n",
      "train loss:0.7612030390244076\n",
      "train loss:0.5501584403951539\n",
      "train loss:0.534294749366209\n",
      "train loss:0.5341625122170737\n",
      "train loss:0.8297686248384519\n",
      "train loss:0.8086430842950534\n",
      "train loss:0.4285011003882511\n",
      "train loss:0.6919042722270227\n",
      "train loss:0.5514471281234326\n",
      "train loss:0.646209319203539\n",
      "train loss:0.5553850191113708\n",
      "train loss:0.5376255514695856\n",
      "train loss:0.7544201966307245\n",
      "train loss:0.6612870468174225\n",
      "train loss:0.5279659539027491\n",
      "train loss:0.538107617429726\n",
      "train loss:0.4200071980291472\n",
      "train loss:0.8306587626623795\n",
      "train loss:0.4110752751334624\n",
      "train loss:0.6758143897391211\n",
      "train loss:0.5398880062717303\n",
      "train loss:0.520618373735521\n",
      "train loss:0.5191209031125398\n",
      "train loss:0.4744353949491165\n",
      "train loss:0.4697696438717223\n",
      "train loss:0.699428124450752\n",
      "train loss:0.7784749992080444\n",
      "train loss:0.6100728715947794\n",
      "train loss:0.5097100782390703\n",
      "train loss:0.33040313055779125\n",
      "train loss:0.7441112372984119\n",
      "train loss:0.7163510459883311\n",
      "train loss:0.6939968779569385\n",
      "train loss:0.6858316667997999\n",
      "train loss:0.6764350609323685\n",
      "train loss:0.4752951936736661\n",
      "train loss:0.3125416267166295\n",
      "train loss:0.49575333766402097\n",
      "train loss:0.5200714102385033\n",
      "train loss:1.1154569797633846\n",
      "train loss:0.4814771307837802\n",
      "train loss:0.4933043330321597\n",
      "train loss:0.29500296872468657\n",
      "train loss:0.6647334280991908\n",
      "train loss:0.6971840265175808\n",
      "train loss:0.27132725825132564\n",
      "train loss:0.7192377609809197\n",
      "train loss:0.2715983326197734\n",
      "train loss:0.7685816425839015\n",
      "train loss:0.30559468030453285\n",
      "train loss:0.23151012075928462\n",
      "train loss:0.9567094372392948\n",
      "train loss:0.4660719074250016\n",
      "train loss:0.6808499245836904\n",
      "train loss:0.9787389780765967\n",
      "train loss:0.4948768586237474\n",
      "train loss:0.5249855798663361\n",
      "train loss:0.7199216348201722\n",
      "train loss:0.27473344313049675\n",
      "train loss:0.494615595818635\n",
      "train loss:0.2981741145313327\n",
      "train loss:0.7678348791646034\n",
      "train loss:0.47835296849010867\n",
      "train loss:0.9481267026999076\n",
      "train loss:0.7155816359120335\n",
      "train loss:0.8752947562152713\n",
      "train loss:0.5033853785256359\n",
      "train loss:0.539529106327685\n",
      "train loss:0.8190462145475557\n",
      "train loss:0.5302634711513361\n",
      "train loss:0.5314387094356257\n",
      "train loss:0.5088366621755777\n",
      "train loss:0.6932390542069468\n",
      "train loss:0.529488726319397\n",
      "train loss:0.391195610948344\n",
      "train loss:0.3782276529494151\n",
      "train loss:0.7303061687162097\n",
      "train loss:0.6871642661912798\n",
      "train loss:0.5379154075251739\n",
      "train loss:0.34854568258719687\n",
      "train loss:0.725588519969437\n",
      "train loss:0.33489857950332935\n",
      "train loss:0.5175289773413064\n",
      "train loss:0.68582636593005\n",
      "train loss:0.2953110518538588\n",
      "train loss:0.7031443191675057\n",
      "train loss:0.7258267357109451\n",
      "train loss:0.6980577929730741\n",
      "train loss:0.9292752004680004\n",
      "train loss:1.084905174271453\n",
      "train loss:0.43933601307979275\n",
      "train loss:0.31676060950688867\n",
      "train loss:0.4945083838569307\n",
      "train loss:0.6657703935824069\n",
      "train loss:0.4880822135003829\n",
      "train loss:0.8737927576771309\n",
      "train loss:0.3285642263497013\n",
      "train loss:1.06553338019853\n",
      "train loss:0.9869145607398668\n",
      "train loss:0.5138156950075559\n",
      "train loss:0.5218946534614028\n",
      "train loss:0.8990771949054132\n",
      "train loss:0.5657362331378193\n",
      "train loss:0.5687407982681724\n",
      "train loss:0.6869002659720274\n",
      "train loss:0.5720165493289089\n",
      "train loss:0.5624782396816395\n",
      "train loss:0.5292167607333712\n",
      "train loss:0.443949812598685\n",
      "train loss:0.7761912913478236\n",
      "train loss:0.6481995433326538\n",
      "train loss:0.4359479406556911\n",
      "train loss:0.4273772627329908\n",
      "train loss:0.4314921006428284\n",
      "train loss:0.4306884342951861\n",
      "train loss:0.7193944934887933\n",
      "train loss:0.6784968379197764\n",
      "train loss:0.5042228429266448\n",
      "train loss:0.8406728862438427\n",
      "train loss:0.6532231076494618\n",
      "train loss:0.5384162099538093\n",
      "train loss:0.7155422174004211\n",
      "train loss:0.5206372794982564\n",
      "train loss:0.52746550311715\n",
      "train loss:0.5206828395183811\n",
      "train loss:0.7161349571439344\n",
      "train loss:0.6763263182240549\n",
      "train loss:0.5521168708344594\n",
      "train loss:0.32803028968854975\n",
      "train loss:0.35244235663861845\n",
      "train loss:0.746356505826838\n",
      "train loss:0.771125440273462\n",
      "train loss:0.5230811111215454\n",
      "train loss:0.7491743642710489\n",
      "train loss:0.32816906826482695\n",
      "train loss:0.4977618382539391\n",
      "train loss:0.5196595036793052\n",
      "train loss:0.5173682338068819\n",
      "train loss:0.5200227489144735\n",
      "train loss:0.46059224590701503\n",
      "train loss:0.7623414796792642\n",
      "train loss:0.4925216388875242\n",
      "train loss:0.5247142315269011\n",
      "train loss:0.26326662739673007\n",
      "train loss:0.7551557544009244\n",
      "train loss:0.24923472897352056\n",
      "train loss:0.8393845292192033\n",
      "train loss:0.48809192826274284\n",
      "train loss:0.2218671723259414\n",
      "train loss:0.6711631530856024\n",
      "train loss:0.513516322378591\n",
      "train loss:0.22605987937582275\n",
      "train loss:0.45592732719036083\n",
      "train loss:0.5555535118415572\n",
      "train loss:0.42733570975776625\n",
      "train loss:0.5202657432617348\n",
      "train loss:0.49067676068406696\n",
      "train loss:0.5012333415433745\n",
      "train loss:1.1336769541154836\n",
      "train loss:0.19536990182102063\n",
      "train loss:1.0882640703689272\n",
      "train loss:0.8037673995387584\n",
      "train loss:0.6880763647270615\n",
      "train loss:0.2932522259012199\n",
      "train loss:0.4944263354418835\n",
      "train loss:0.5289239308565583\n",
      "train loss:0.3224595221265923\n",
      "train loss:0.7610996269182679\n",
      "train loss:1.0760011089240487\n",
      "train loss:0.33938026431077645\n",
      "train loss:0.7133335900189234\n",
      "train loss:0.6967652658818958\n",
      "train loss:0.6029998517584044\n",
      "train loss:0.37726412867151693\n",
      "train loss:0.5530833574478791\n",
      "train loss:0.5546129073972075\n",
      "train loss:0.6883934917470542\n",
      "train loss:0.6724149006178367\n",
      "train loss:0.6851213842176226\n",
      "train loss:0.3505423688213445\n",
      "train loss:0.5377752622601448\n",
      "train loss:0.527495616945418\n",
      "train loss:0.5229701682133315\n",
      "train loss:0.856088048586809\n",
      "train loss:0.3526657364185599\n",
      "train loss:0.5081671104264434\n",
      "train loss:0.5128609605141887\n",
      "train loss:0.7049645972241929\n",
      "train loss:0.4816688970334958\n",
      "train loss:0.3209482113028796\n",
      "train loss:0.506551528077761\n",
      "train loss:0.3062729650234721\n",
      "train loss:0.5380221730413821\n",
      "train loss:0.7018997168431181\n",
      "train loss:0.25705100787458895\n",
      "train loss:0.7207874685837339\n",
      "train loss:0.23259017149057898\n",
      "train loss:1.0661591245968804\n",
      "train loss:0.6858219454817889\n",
      "train loss:0.7831149928545129\n",
      "train loss:0.7672677028196292\n",
      "train loss:0.976273361400332\n",
      "train loss:0.4938932751982753\n",
      "train loss:0.47412573406925834\n",
      "train loss:0.7247096916274609\n",
      "train loss:0.6930903771143025\n",
      "train loss:0.5104589643255578\n",
      "train loss:0.6768676362462656\n",
      "train loss:0.8786820566884789\n",
      "train loss:0.36598216042816023\n",
      "train loss:0.49881590384462804\n",
      "train loss:0.5145402864199353\n",
      "train loss:0.48416484965380907\n",
      "train loss:0.567894480680093\n",
      "train loss:0.6767601329450202\n",
      "train loss:0.6989702254233583\n",
      "train loss:0.5316014080950313\n",
      "train loss:0.663567975138075\n",
      "train loss:0.37841198446234536\n",
      "train loss:0.6618821836430737\n",
      "train loss:0.8017580602630525\n",
      "train loss:0.5296049338307143\n",
      "train loss:0.6228856925628625\n",
      "train loss:0.8100494323469828\n",
      "train loss:0.863895339745698\n",
      "train loss:0.6790229004431652\n",
      "train loss:0.670187958837587\n",
      "train loss:0.7355059953489855\n",
      "train loss:0.43960087926169955\n",
      "train loss:0.5454721543601464\n",
      "train loss:0.4070950205845126\n",
      "train loss:0.3600024196454886\n",
      "train loss:0.7985732019000669\n",
      "train loss:0.37985711587604065\n",
      "train loss:0.5686556728521477\n",
      "train loss:0.3611794721805346\n",
      "train loss:0.514181605886842\n",
      "train loss:0.5614557410921689\n",
      "train loss:0.8609936141223125\n",
      "train loss:0.4913718852068964\n",
      "train loss:0.4887435094440642\n",
      "train loss:0.5196171405644339\n",
      "train loss:0.7252838444685286\n",
      "train loss:0.6685762585801085\n",
      "train loss:0.6902366758292721\n",
      "train loss:0.6709716324500775\n",
      "train loss:0.5197524435452976\n",
      "train loss:0.5094797472620995\n",
      "train loss:0.49443666583375173\n",
      "train loss:0.5152905039376486\n",
      "train loss:0.493661186497257\n",
      "train loss:0.7117542533713113\n",
      "train loss:0.7228615790817077\n",
      "train loss:1.0047672972296582\n",
      "train loss:0.5139619105432554\n",
      "train loss:0.5640916452053562\n",
      "train loss:0.6697581631220096\n",
      "train loss:0.672619522594972\n",
      "train loss:0.5361429441093418\n",
      "train loss:0.35473706049160975\n",
      "train loss:0.9267529642216813\n",
      "train loss:0.36146196784799367\n",
      "train loss:0.6794665472333266\n",
      "train loss:0.34981438162017253\n",
      "train loss:0.8473633068680637\n",
      "train loss:0.6560268030531856\n",
      "train loss:0.9245896178707879\n",
      "train loss:0.5019579823870577\n",
      "train loss:0.5060038010736011\n",
      "train loss:0.6995357009818903\n",
      "train loss:0.34387325901598326\n",
      "train loss:0.9449353892449217\n",
      "train loss:0.8214732218132493\n",
      "train loss:0.7224920651390437\n",
      "train loss:0.5353668222912902\n",
      "train loss:0.5451306856188347\n",
      "train loss:0.6486432149305755\n",
      "train loss:0.4251086552578407\n",
      "train loss:0.6390214145026207\n",
      "train loss:0.5415094647209227\n",
      "train loss:0.6833590074400558\n",
      "train loss:0.3888689092480796\n",
      "train loss:0.5274978614879698\n",
      "train loss:0.6743509026786094\n",
      "train loss:0.6694988433260884\n",
      "train loss:0.6698273497982182\n",
      "train loss:0.5312637750332755\n",
      "train loss:0.5557495046478144\n",
      "train loss:0.6930969429682861\n",
      "train loss:0.37598911174049865\n",
      "train loss:0.5062889058286363\n",
      "train loss:0.4940753085805841\n",
      "train loss:0.9056901725023294\n",
      "train loss:0.6601230457570799\n",
      "train loss:0.702267620437756\n",
      "train loss:0.69444075345646\n",
      "train loss:0.8770172998593931\n",
      "train loss:0.3380193833444788\n",
      "train loss:0.6728584732492573\n",
      "train loss:0.5532791757418136\n",
      "train loss:0.3603242799006696\n",
      "train loss:0.6529847285358843\n",
      "train loss:0.3493815204664382\n",
      "train loss:0.5208811949011012\n",
      "train loss:0.3583752094877583\n",
      "train loss:0.5127521546067412\n",
      "train loss:0.5287641117609316\n",
      "train loss:0.7115744006690179\n",
      "train loss:0.5871941438409827\n",
      "train loss:0.682474051378669\n",
      "train loss:0.7486611802664564\n",
      "train loss:0.325150716367639\n",
      "train loss:0.4961839879695785\n",
      "train loss:0.2788177956108157\n",
      "train loss:0.9395345879040837\n",
      "train loss:0.807994159278756\n",
      "train loss:0.2852445889487562\n",
      "train loss:0.2796980879039067\n",
      "train loss:0.47262571066671644\n",
      "train loss:0.6861715306281547\n",
      "train loss:0.6966962758859466\n",
      "train loss:0.9110076459784056\n",
      "train loss:0.4971247696504008\n",
      "train loss:0.522587827080361\n",
      "train loss:0.7084388706964506\n",
      "train loss:0.27554953080309597\n",
      "train loss:0.4425029027195251\n",
      "train loss:0.661469356729705\n",
      "train loss:1.0277427635431216\n",
      "train loss:0.5875199040760835\n",
      "train loss:0.9289279391139296\n",
      "train loss:0.5315636752990454\n",
      "train loss:0.5368028549169257\n",
      "train loss:0.6648052002378245\n",
      "train loss:0.398446462560003\n",
      "train loss:0.6603456152706747\n",
      "train loss:0.7999287010352066\n",
      "train loss:0.7158949885259359\n",
      "train loss:0.789444277962605\n",
      "train loss:0.42019511699979456\n",
      "train loss:0.6783032325208006\n",
      "train loss:0.7424751643433823\n",
      "train loss:0.5747957206051357\n",
      "train loss:0.8021781923494048\n",
      "train loss:0.5633036062893179\n",
      "train loss:0.85248428496112\n",
      "train loss:0.6683907390694142\n",
      "train loss:0.46560817505914737\n",
      "train loss:0.6650763818806216\n",
      "train loss:0.5739799311964194\n",
      "train loss:0.7576391262851184\n",
      "train loss:0.5785989826128459\n",
      "train loss:0.48378343242281163\n",
      "train loss:0.5911610283228148\n",
      "train loss:0.7534011601834424\n",
      "train loss:0.5351676050873769\n",
      "train loss:0.6845127457719632\n",
      "train loss:0.6862371331010781\n",
      "train loss:0.5907976644576463\n",
      "train loss:0.5695619437349778\n",
      "train loss:0.680568448372668\n",
      "train loss:0.664830138205571\n",
      "train loss:0.7161570000619184\n",
      "train loss:0.5526150384067454\n",
      "train loss:0.42889641668773715\n",
      "train loss:0.5482108274534533\n",
      "train loss:0.40557824240582663\n",
      "train loss:0.8012760382223105\n",
      "train loss:0.3992954547538461\n",
      "train loss:0.5443273237209785\n",
      "train loss:0.3875980538240889\n",
      "train loss:0.34784088821548953\n",
      "train loss:0.7311634382073398\n",
      "train loss:0.4876837826193496\n",
      "train loss:0.5222914962906894\n",
      "train loss:0.6988202201414102\n",
      "train loss:1.123820829203589\n",
      "train loss:0.31114493308468016\n",
      "train loss:0.29995419663385314\n",
      "train loss:0.7357470080895324\n",
      "train loss:0.48743421637180767\n",
      "train loss:0.48109849420346534\n",
      "train loss:0.9974287487623489\n",
      "train loss:0.5146718143083192\n",
      "train loss:0.9148508579944729\n",
      "train loss:0.722939007508282\n",
      "train loss:0.5154753703501884\n",
      "train loss:0.5093624070175153\n",
      "train loss:0.6841328186484078\n",
      "train loss:0.30674677682381224\n",
      "train loss:0.508846372867914\n",
      "train loss:0.49754415843763605\n",
      "train loss:0.663007098708486\n",
      "train loss:0.68586078927174\n",
      "train loss:0.7036548495892746\n",
      "train loss:0.6938962347609383\n",
      "train loss:0.5637346378802895\n",
      "train loss:0.6864113819628297\n",
      "train loss:0.6808101144889961\n",
      "train loss:0.5308508836695138\n",
      "train loss:0.8715490869305924\n",
      "train loss:0.6868622880049177\n",
      "train loss:0.5443310302718584\n",
      "train loss:0.38741006008737366\n",
      "train loss:0.7033830881097554\n",
      "train loss:0.4688305259390818\n",
      "train loss:0.6896576936718979\n",
      "train loss:0.4909064774851255\n",
      "train loss:0.6618020201860755\n",
      "train loss:0.3788070007577424\n",
      "train loss:0.6537635063510809\n",
      "train loss:0.3201425864580985\n",
      "train loss:0.5797429367065731\n",
      "train loss:0.8568980328137142\n",
      "train loss:0.6919660273165954\n",
      "train loss:0.5048937259691253\n",
      "train loss:1.0171047279713588\n",
      "train loss:0.6737639284956647\n",
      "train loss:0.5510063577249809\n",
      "train loss:0.5546406536608932\n",
      "train loss:0.6736862483810603\n",
      "train loss:0.5256161315397497\n",
      "train loss:0.5786215220004113\n",
      "train loss:0.586851065400095\n",
      "train loss:0.5222355767469702\n",
      "train loss:0.5032948714005748\n",
      "train loss:0.5448705767155585\n",
      "train loss:1.087321445066563\n",
      "train loss:0.4982648410208045\n",
      "train loss:0.9122121525531451\n",
      "train loss:0.3943234941252906\n",
      "train loss:0.41050390152455185\n",
      "train loss:0.5363496030033106\n",
      "train loss:0.509493061659299\n",
      "train loss:0.5098269221259101\n",
      "train loss:0.7071482389067744\n",
      "train loss:0.3943196913627483\n",
      "train loss:0.5341135378543604\n",
      "train loss:0.6832132631044888\n",
      "train loss:0.6551753849037787\n",
      "train loss:0.6332216433581002\n",
      "train loss:0.5195677365251854\n",
      "train loss:0.361341290313243\n",
      "train loss:0.5501831091459728\n",
      "train loss:0.720336843607853\n",
      "train loss:0.6207738002024644\n",
      "train loss:0.6900435856271881\n",
      "train loss:0.5181140136984311\n",
      "train loss:0.4939794595924111\n",
      "train loss:0.6733061227222117\n",
      "train loss:0.5550888266261393\n",
      "train loss:0.681853973205411\n",
      "train loss:0.3226319039573629\n",
      "train loss:0.7840777753052917\n",
      "train loss:0.7366991825311305\n",
      "train loss:0.5296588485303222\n",
      "train loss:0.7729149462099181\n",
      "train loss:0.6985268280271004\n",
      "train loss:0.28125691467068525\n",
      "train loss:0.5059441633657039\n",
      "train loss:0.6917924021814699\n",
      "train loss:0.8499262587683605\n",
      "train loss:0.5363057651928648\n",
      "train loss:0.33165805196700154\n",
      "train loss:0.49105946823388946\n",
      "train loss:0.6894830629420762\n",
      "train loss:0.9515813478550014\n",
      "train loss:0.5200748391721375\n",
      "train loss:0.7068666028727291\n",
      "train loss:0.8525382651069346\n",
      "train loss:0.5126694204938096\n",
      "train loss:0.5422244843436452\n",
      "train loss:0.5067489591509208\n",
      "train loss:0.6623202237414667\n",
      "train loss:0.6676763707654386\n",
      "train loss:0.6401549814865062\n",
      "train loss:0.5492284104045785\n",
      "train loss:0.6663875976200149\n",
      "train loss:0.40613509985036844\n",
      "train loss:0.5217457559994834\n",
      "train loss:0.5372793407923344\n",
      "train loss:0.41902673724374767\n",
      "train loss:0.49628079869853803\n",
      "train loss:0.535468833584005\n",
      "train loss:0.6070993964198748\n",
      "train loss:1.1920171399160182\n",
      "train loss:0.7767220299380407\n",
      "train loss:0.3682362683769553\n",
      "train loss:0.7932102515451787\n",
      "train loss:0.5749098270186124\n",
      "train loss:0.6987710339707527\n",
      "train loss:0.5226047965953067\n",
      "train loss:0.5118047785319245\n",
      "train loss:0.3776697711953453\n",
      "train loss:0.7965718157206114\n",
      "train loss:0.5180870900308681\n",
      "train loss:0.5088401526411304\n",
      "train loss:0.735517230022201\n",
      "train loss:0.5372040382919123\n",
      "train loss:0.5338145498211194\n",
      "train loss:0.5855886161483662\n",
      "train loss:0.6960234416683961\n",
      "train loss:0.7032301719392393\n",
      "train loss:0.8649658489063656\n",
      "train loss:0.5074776824173037\n",
      "train loss:0.6742643298465925\n",
      "train loss:0.5499922329825003\n",
      "train loss:0.6786810408330108\n",
      "train loss:0.5144534156377794\n",
      "train loss:0.5295933756812409\n",
      "train loss:0.708754629716003\n",
      "train loss:0.6415473057862627\n",
      "train loss:0.7184917669025576\n",
      "train loss:0.8720762332810834\n",
      "train loss:0.3998476288024531\n",
      "train loss:0.4046208563075811\n",
      "train loss:0.532688992535983\n",
      "train loss:0.3727739604999486\n",
      "train loss:0.6893699538181597\n",
      "train loss:0.36643445510406353\n",
      "train loss:0.6865371208854676\n",
      "train loss:0.5552026401342935\n",
      "train loss:0.7159039543299013\n",
      "train loss:0.8799988552005779\n",
      "train loss:0.5424536643812068\n",
      "train loss:0.3248330194762513\n",
      "train loss:0.6985478834368832\n",
      "train loss:0.4984494119456319\n",
      "train loss:0.4641613712645357\n",
      "train loss:0.6950661227857953\n",
      "train loss:0.7433226032963642\n",
      "train loss:0.31987843953334744\n",
      "train loss:0.8990291622854277\n",
      "train loss:0.5267306487340737\n",
      "train loss:0.8932802456498304\n",
      "train loss:0.5071733908500252\n",
      "train loss:0.6770525098811945\n",
      "train loss:0.4929394587650253\n",
      "train loss:1.036632079280505\n",
      "train loss:0.695517171156005\n",
      "train loss:0.6408729851119161\n",
      "train loss:0.6849583205568791\n",
      "train loss:0.38784862896970573\n",
      "train loss:0.3721339668777312\n",
      "train loss:0.695705745069953\n",
      "train loss:0.6763362398485195\n",
      "train loss:0.5627918051020823\n",
      "train loss:0.8278230794416164\n",
      "train loss:0.5346593276700915\n",
      "train loss:0.5121543845630894\n",
      "train loss:0.37273090428434213\n",
      "train loss:0.39131634496829026\n",
      "train loss:0.7118611682394625\n",
      "train loss:0.5819485069265049\n",
      "train loss:0.40996569917771086\n",
      "train loss:0.7046492285563712\n",
      "train loss:0.8342311803304779\n",
      "train loss:0.6366843159654361\n",
      "train loss:0.5197735268967818\n",
      "train loss:0.7356975323346023\n",
      "train loss:0.6357478426893453\n",
      "train loss:0.3665980286873888\n",
      "train loss:0.867859704247358\n",
      "train loss:0.5363099538423988\n",
      "train loss:0.4759323149482354\n",
      "train loss:0.8295233133641531\n",
      "train loss:0.6990356742044708\n",
      "train loss:0.8516401553073416\n",
      "train loss:0.8888812698182734\n",
      "train loss:0.9098646208922787\n",
      "train loss:0.6758289849281203\n",
      "train loss:0.5292926638164239\n",
      "train loss:0.7976022114394129\n",
      "train loss:0.5706629060859136\n",
      "train loss:0.5111757978773153\n",
      "train loss:0.5657792344208008\n",
      "train loss:0.4623887364897401\n",
      "train loss:0.6749480068662412\n",
      "train loss:0.7918086480890922\n",
      "train loss:0.5652159412459047\n",
      "train loss:0.7653161860681463\n",
      "train loss:0.5777606577081518\n",
      "train loss:0.6653402856188221\n",
      "train loss:0.8946501853486157\n",
      "train loss:0.6748466632091386\n",
      "train loss:0.48461555200630946\n",
      "train loss:0.939530548184865\n",
      "train loss:0.5893529518007081\n",
      "train loss:0.6760893982389777\n",
      "train loss:0.5749169254309038\n",
      "train loss:0.6724043614463802\n",
      "train loss:0.8074215583025574\n",
      "train loss:0.49543517077822774\n",
      "train loss:0.6745683111359101\n",
      "train loss:0.5593236620285015\n",
      "train loss:0.5895363453666846\n",
      "train loss:0.5605754347194185\n",
      "train loss:0.6400947283362961\n",
      "train loss:0.7456997817717262\n",
      "train loss:0.6676807152997123\n",
      "train loss:0.6696841551051587\n",
      "train loss:0.5595793778905599\n",
      "train loss:0.5585580935546987\n",
      "train loss:0.4326900942822748\n",
      "train loss:0.558177145582507\n",
      "train loss:0.5185582768778602\n",
      "train loss:0.5304176581784235\n",
      "train loss:0.690815431864854\n",
      "train loss:0.5413195763620704\n",
      "train loss:0.7959720655965519\n",
      "train loss:0.5257523912194715\n",
      "train loss:0.41122846847445665\n",
      "train loss:0.5702071697915982\n",
      "train loss:0.5453057272497525\n",
      "train loss:0.37946588344712423\n",
      "train loss:0.9644981585361304\n",
      "train loss:0.5458817716105026\n",
      "train loss:0.6770053936647469\n",
      "train loss:0.5038144843091915\n",
      "train loss:0.3293842925485492\n",
      "train loss:0.6884380447820819\n",
      "train loss:0.6797534015030806\n",
      "train loss:0.7044985557561148\n",
      "train loss:0.4693054664027872\n",
      "train loss:0.5304025071342455\n",
      "train loss:0.6893458059926585\n",
      "train loss:0.5277029966389614\n",
      "train loss:0.5498340797901172\n",
      "train loss:0.514833852659527\n",
      "train loss:0.29532701786112664\n",
      "train loss:0.29080709203058563\n",
      "train loss:0.718835752021932\n",
      "train loss:0.5224556989819129\n",
      "train loss:0.27579485171549123\n",
      "train loss:0.5038485558219186\n",
      "train loss:0.5004392466030743\n",
      "train loss:0.5340898386947004\n",
      "train loss:0.473460475598002\n",
      "train loss:0.2098950269823549\n",
      "train loss:0.6960716445393734\n",
      "train loss:0.8485964622443658\n",
      "train loss:0.8185983135508021\n",
      "train loss:0.45769694472402217\n",
      "train loss:0.213098163313545\n",
      "train loss:0.4976701399652389\n",
      "train loss:0.5295124423294578\n",
      "train loss:0.741917396055225\n",
      "train loss:0.5837297674241747\n",
      "train loss:0.2139834665383682\n",
      "train loss:0.7974281356763686\n",
      "train loss:0.539749047932409\n",
      "train loss:0.4734047718997541\n",
      "train loss:1.4091428370072907\n",
      "train loss:0.7875407287032805\n",
      "train loss:0.2614082678318927\n",
      "train loss:0.31797118581020467\n",
      "train loss:0.3160448926022835\n",
      "train loss:0.5218410601749802\n",
      "train loss:0.688137923764941\n",
      "train loss:0.6867032369512123\n",
      "train loss:0.5766753402663707\n",
      "train loss:0.3603435689709199\n",
      "train loss:0.3424523445689346\n",
      "train loss:0.5349028763261738\n",
      "train loss:0.5417777714077722\n",
      "train loss:0.5318837360261582\n",
      "train loss:0.3972638009109158\n",
      "train loss:0.5211728272374553\n",
      "train loss:0.712395551324805\n",
      "train loss:0.5764080804477663\n",
      "train loss:0.2598128427281177\n",
      "train loss:1.1499848646237343\n",
      "train loss:0.7634197414389511\n",
      "train loss:0.7304211746988323\n",
      "train loss:0.5028301997953493\n",
      "train loss:0.9378126089265406\n",
      "train loss:0.35222552964683346\n",
      "train loss:0.7083604355791838\n",
      "train loss:0.5136663829476245\n",
      "train loss:0.5257430586843582\n",
      "train loss:0.5346344711927035\n",
      "train loss:0.7038593963750862\n",
      "train loss:0.6793690031205959\n",
      "train loss:0.4916683172524861\n",
      "train loss:0.6775843407779645\n",
      "train loss:0.5313648074168398\n",
      "train loss:0.9821559307309411\n",
      "train loss:0.6659838125719063\n",
      "train loss:0.6654340876869134\n",
      "train loss:0.6583415835208719\n",
      "train loss:0.5579276346331373\n",
      "train loss:0.5078123958027141\n",
      "train loss:0.8507607813382826\n",
      "train loss:0.542273004719973\n",
      "train loss:0.5207480861796905\n",
      "train loss:0.8376906691779966\n",
      "train loss:1.0554939357182973\n",
      "train loss:0.5516540426099563\n",
      "train loss:0.562458831238259\n",
      "train loss:0.792116637142509\n",
      "train loss:0.5608107541883858\n",
      "train loss:0.4325342963772618\n",
      "train loss:0.629158736419312\n",
      "train loss:0.7920684113254947\n",
      "train loss:0.4457614678917642\n",
      "train loss:0.5621109290356363\n",
      "train loss:0.6842859165933507\n",
      "train loss:0.5662898303541413\n",
      "train loss:0.6871520248711412\n",
      "train loss:0.6684596724616239\n",
      "train loss:0.4321310650935307\n",
      "train loss:0.6729146021457133\n",
      "train loss:0.546146816925034\n",
      "train loss:0.547717753579999\n",
      "train loss:0.5391960933025185\n",
      "train loss:0.6795960757268391\n",
      "train loss:0.655205370776504\n",
      "train loss:0.7161277199901821\n",
      "train loss:0.5248975677183363\n",
      "train loss:0.5214998996510107\n",
      "train loss:0.9640575548302415\n",
      "train loss:0.4294925762467606\n",
      "train loss:0.5250651425188896\n",
      "train loss:0.5186755621679231\n",
      "train loss:0.6812130302477561\n",
      "train loss:0.6820984145895094\n",
      "train loss:0.6353701602274805\n",
      "train loss:0.6952819501800468\n",
      "train loss:0.5414863894114943\n",
      "train loss:0.7634020245614558\n",
      "train loss:0.546243404774511\n",
      "train loss:0.775637897834586\n",
      "train loss:0.5660398972016126\n",
      "train loss:0.5357458076455449\n",
      "train loss:0.9868725152317062\n",
      "train loss:0.6722190716500979\n",
      "train loss:0.9478958618355099\n",
      "train loss:0.6780696544290439\n",
      "train loss:0.6136049835467048\n",
      "train loss:0.9250465057019476\n",
      "train loss:0.6650116370136401\n",
      "train loss:0.4299523807192708\n",
      "train loss:0.7594730369900151\n",
      "train loss:0.7846422563583209\n",
      "train loss:0.6717469709162007\n",
      "train loss:0.5761472725336599\n",
      "train loss:0.5800600377771014\n",
      "train loss:0.6697652019802998\n",
      "train loss:0.6699532307244584\n",
      "train loss:0.6699181390538167\n",
      "train loss:0.5942817001143149\n",
      "train loss:0.7597270412473485\n",
      "train loss:0.7537085859195346\n",
      "train loss:0.504477215317461\n",
      "train loss:0.5908771100220427\n",
      "train loss:0.5883343491195716\n",
      "train loss:0.5001698275489849\n",
      "train loss:0.5542212213290503\n",
      "train loss:0.8028755880079264\n",
      "train loss:0.5909541425233256\n",
      "train loss:0.5651768204249383\n",
      "train loss:0.6443190169429489\n",
      "train loss:0.8896497623265729\n",
      "train loss:0.47629607950571246\n",
      "train loss:0.46077146157180177\n",
      "train loss:0.5720634047503476\n",
      "train loss:0.5387154323321262\n",
      "train loss:0.5665810691269964\n",
      "train loss:0.552454700406037\n",
      "train loss:0.8175881203163889\n",
      "train loss:0.5356829781398733\n",
      "train loss:0.42079966984751815\n",
      "train loss:0.681667135295706\n",
      "train loss:0.5299605358554857\n",
      "train loss:0.3900996683412984\n",
      "train loss:0.7857071218692295\n",
      "train loss:0.5406476491989498\n",
      "train loss:0.5276798346864818\n",
      "train loss:0.5597950667582975\n",
      "train loss:1.0167614906975904\n",
      "train loss:0.330193526144596\n",
      "train loss:0.383648169899356\n",
      "train loss:0.6981910557313463\n",
      "train loss:0.6805532494880033\n",
      "train loss:0.7095702009652316\n",
      "train loss:0.3071012322424734\n",
      "train loss:0.5090099683547648\n",
      "train loss:0.7143674911705173\n",
      "train loss:0.721335104535535\n",
      "train loss:0.7162138686935655\n",
      "train loss:0.48997777594802094\n",
      "train loss:0.5167748959608025\n",
      "train loss:0.5333266348573262\n",
      "train loss:0.3046140670062087\n",
      "train loss:0.48256369679264066\n",
      "train loss:0.5807803600761966\n",
      "train loss:0.7142466640339308\n",
      "train loss:0.3014166464711776\n",
      "train loss:0.43873478659717496\n",
      "train loss:0.5271273639048416\n",
      "train loss:0.7261816084223949\n",
      "train loss:0.4961177530635391\n",
      "train loss:0.733993340216925\n",
      "train loss:0.7084626108944787\n",
      "train loss:0.7195103484966141\n",
      "train loss:0.7487768414848094\n",
      "train loss:0.5105413113374642\n",
      "train loss:0.5231906464472773\n",
      "train loss:0.9204238609021989\n",
      "train loss:0.7054531091300604\n",
      "train loss:0.6882487900133258\n",
      "train loss:0.8668822143138211\n",
      "train loss:0.6723862503153925\n",
      "train loss:0.3656441843290522\n",
      "train loss:0.8113349351488823\n",
      "train loss:0.36363829307623474\n",
      "train loss:0.6873974162357375\n",
      "train loss:0.7333852042877869\n",
      "train loss:0.8084335768018468\n",
      "train loss:0.6830400109861644\n",
      "train loss:0.5501918922304052\n",
      "train loss:0.6757550835177313\n",
      "train loss:0.8793773486694747\n",
      "train loss:0.41044522364223746\n",
      "train loss:0.5214922123822336\n",
      "train loss:0.4750511080027319\n",
      "train loss:0.5845994050082421\n",
      "train loss:0.6592449620065148\n",
      "train loss:0.6344589000872961\n",
      "train loss:0.44669660239429454\n",
      "train loss:0.6212052192838664\n",
      "train loss:0.5637975112207229\n",
      "train loss:0.5210404409789181\n",
      "train loss:0.5397225570526977\n",
      "train loss:0.788369344482198\n",
      "train loss:0.6639889916277877\n",
      "train loss:0.6005618253334547\n",
      "train loss:0.9400965236003038\n",
      "train loss:0.49378988222804737\n",
      "train loss:0.8514860872355305\n",
      "train loss:0.5447011171698983\n",
      "train loss:0.6881201436007592\n",
      "train loss:0.9384786808923801\n",
      "train loss:0.5611399996125497\n",
      "train loss:0.5419018021674746\n",
      "train loss:0.9133027034878254\n",
      "train loss:0.8972713612263451\n",
      "train loss:0.5727821328002853\n",
      "train loss:0.40318943721997336\n",
      "train loss:0.564182044657267\n",
      "train loss:0.5634167454519381\n",
      "train loss:0.6823887240528955\n",
      "train loss:0.5672932114730557\n",
      "train loss:0.6783435740209721\n",
      "train loss:0.6704110858762256\n",
      "train loss:0.558848225939357\n",
      "train loss:0.40553210485064517\n",
      "train loss:0.6831667205265605\n",
      "train loss:0.6719408642271956\n",
      "train loss:0.5372490813722509\n",
      "train loss:0.5736205911955766\n",
      "train loss:0.4200027850557423\n",
      "train loss:0.6562083014217449\n",
      "train loss:0.7954169469718693\n",
      "train loss:0.5452708279256232\n",
      "train loss:0.7823155748550775\n",
      "train loss:0.7063343757343248\n",
      "train loss:0.5613560987690813\n",
      "train loss:0.627343602010825\n",
      "train loss:0.6602514979008436\n",
      "train loss:0.4044991868442451\n",
      "train loss:0.5408887524008179\n",
      "train loss:0.6451850184807325\n",
      "train loss:0.5464330507450366\n",
      "train loss:0.3618745442506825\n",
      "train loss:0.5093454370068669\n",
      "train loss:0.4992464140529713\n",
      "train loss:0.31510783317316216\n",
      "train loss:0.9114855118824592\n",
      "train loss:0.5466307659512625\n",
      "train loss:0.3237283587235097\n",
      "train loss:0.7106627161423921\n",
      "train loss:0.6828821770679531\n",
      "train loss:0.5809635091906713\n",
      "train loss:0.6939447621750956\n",
      "train loss:0.5041224497492391\n",
      "train loss:0.3063873313654062\n",
      "train loss:0.4946572191073993\n",
      "train loss:0.9391339848555148\n",
      "train loss:0.2885702970899703\n",
      "train loss:0.46854413634769526\n",
      "train loss:0.505188933882711\n",
      "train loss:0.5096852579519677\n",
      "train loss:0.7166877724510611\n",
      "train loss:0.569406146513099\n",
      "train loss:0.4795570748667065\n",
      "train loss:0.5422613870837796\n",
      "train loss:0.5190769907392143\n",
      "train loss:0.6612127457024201\n",
      "train loss:0.888764942299607\n",
      "train loss:0.49878973365256263\n",
      "train loss:0.2461894335189057\n",
      "train loss:0.7403942945085895\n",
      "train loss:0.6680639881338322\n",
      "train loss:0.26407324534135035\n",
      "train loss:0.7473539397489295\n",
      "train loss:1.1423632926593301\n",
      "train loss:0.5320307363898127\n",
      "train loss:0.26413662705952173\n",
      "train loss:0.4770568299487515\n",
      "train loss:0.5505729921189451\n",
      "train loss:0.5024905572513798\n",
      "train loss:0.6879679749563729\n",
      "train loss:0.7116796697880221\n",
      "train loss:0.7124414896244976\n",
      "train loss:0.5045863233186605\n",
      "train loss:0.8652160320992188\n",
      "train loss:0.5237566336520448\n",
      "train loss:0.5131212101869956\n",
      "train loss:0.8964394092057685\n",
      "train loss:0.6571396110007429\n",
      "train loss:0.6976882212783787\n",
      "train loss:0.6605449418764407\n",
      "train loss:0.5551016170136754\n",
      "train loss:0.4833346301351297\n",
      "train loss:0.7106619257075124\n",
      "train loss:0.5808013779930048\n",
      "train loss:0.49189164882865677\n",
      "train loss:0.5632541710894773\n",
      "train loss:0.8396615864928105\n",
      "train loss:0.6800305357139874\n",
      "train loss:0.4891018689580829\n",
      "train loss:0.5897037453987595\n",
      "train loss:0.41901199881712703\n",
      "train loss:0.6795412305867545\n",
      "train loss:0.5510350486227406\n",
      "train loss:0.3971736809807497\n",
      "train loss:0.7946909501320554\n",
      "train loss:0.7085384543965346\n",
      "train loss:0.8321354616921897\n",
      "train loss:0.5245677066047929\n",
      "train loss:0.3391208573551952\n",
      "train loss:0.3898100984859874\n",
      "train loss:0.3915997690144605\n",
      "train loss:0.6879364243715931\n",
      "train loss:0.7009325909229213\n",
      "train loss:0.6763948568784361\n",
      "train loss:0.5070322649692469\n",
      "train loss:0.5753372031192543\n",
      "train loss:0.6718792368542982\n",
      "train loss:0.4914547656520426\n",
      "train loss:0.5285802203122568\n",
      "train loss:0.5096938308517307\n",
      "train loss:0.723453909987337\n",
      "train loss:0.8953862864931589\n",
      "train loss:0.48273663980005715\n",
      "train loss:0.3203276649377547\n",
      "train loss:0.5214441222115327\n",
      "train loss:0.5288022135755021\n",
      "train loss:0.5117886228910835\n",
      "train loss:0.5108730919180836\n",
      "train loss:0.4772126907540254\n",
      "train loss:0.28527544491593665\n",
      "train loss:0.673100577378525\n",
      "train loss:0.5061910504601792\n",
      "train loss:0.7343228620551355\n",
      "train loss:0.4831527303974788\n",
      "train loss:0.49733774186318114\n",
      "train loss:0.26645260317733094\n",
      "train loss:0.7603724761594507\n",
      "train loss:0.5140106692179149\n",
      "train loss:0.5159497281919957\n",
      "train loss:1.0976787725635861\n",
      "train loss:0.7654863394449931\n",
      "train loss:0.7502272941507575\n",
      "train loss:0.7912102387384073\n",
      "train loss:0.6481214986220349\n",
      "train loss:0.7171483350476657\n",
      "train loss:0.7480682195424058\n",
      "train loss:0.7356251332855355\n",
      "train loss:0.6802598748932349\n",
      "train loss:0.6969848209437803\n",
      "train loss:0.522813874510525\n",
      "train loss:0.6964856349557698\n",
      "train loss:0.3916434964970706\n",
      "train loss:0.39402659284951264\n",
      "train loss:0.7680056389293715\n",
      "train loss:0.3887189545124469\n",
      "train loss:0.5267411123136615\n",
      "train loss:0.38488506930641025\n",
      "train loss:0.9797122396165572\n",
      "train loss:0.6585008111118063\n",
      "train loss:0.5309775237915866\n",
      "train loss:0.40265896810759416\n",
      "train loss:0.7363779084008041\n",
      "train loss:0.8159503024298299\n",
      "train loss:0.7609918667935792\n",
      "train loss:0.9609488886371095\n",
      "train loss:0.8252014376058447\n",
      "train loss:0.5381927745064186\n",
      "train loss:0.4150239833461279\n",
      "train loss:0.5404266338458592\n",
      "train loss:0.6873553304187274\n",
      "train loss:0.6867840477883903\n",
      "train loss:0.42955206958343073\n",
      "train loss:0.555666116765239\n",
      "train loss:0.6651678093255378\n",
      "train loss:0.5414070507983736\n",
      "train loss:0.7813476847211568\n",
      "train loss:0.8059284993908455\n",
      "train loss:0.5481523317470092\n",
      "train loss:0.5431452441263038\n",
      "train loss:0.6628607643351211\n",
      "train loss:0.5504948120607068\n",
      "train loss:0.5421807001310454\n",
      "train loss:0.349021850981622\n",
      "train loss:0.684736381980654\n",
      "train loss:0.7783237374169001\n",
      "train loss:0.5330029105783518\n",
      "train loss:0.40914603049335474\n",
      "train loss:0.6784536098772589\n",
      "train loss:0.545617657980937\n",
      "train loss:0.5202904067664342\n",
      "train loss:0.9504706608991622\n",
      "train loss:0.3859576607360903\n",
      "train loss:0.665617466258001\n",
      "train loss:0.33143304113766464\n",
      "train loss:0.8172700062217448\n",
      "train loss:0.37103753525538236\n",
      "train loss:0.6697583882774049\n",
      "train loss:0.6875602255980574\n",
      "train loss:0.5300672813094323\n",
      "train loss:0.5407178635633535\n",
      "train loss:0.5760634437814047\n",
      "train loss:0.46007478729230555\n",
      "train loss:0.32314621620222433\n",
      "train loss:0.5144089536743435\n",
      "train loss:0.48463846556510404\n",
      "train loss:0.5051440903680515\n",
      "train loss:0.669088182324851\n",
      "train loss:0.7451199669891688\n",
      "train loss:0.9369805130114844\n",
      "train loss:0.47027960901859006\n",
      "train loss:0.7227133742424636\n",
      "train loss:0.7202194630998353\n",
      "train loss:0.3075008306107344\n",
      "train loss:0.5098405664107608\n",
      "train loss:0.6839219951454556\n",
      "train loss:0.26134728223033876\n",
      "train loss:0.5006823011757271\n",
      "train loss:0.26225212858244495\n",
      "train loss:0.9264499813750824\n",
      "train loss:0.4871481031315946\n",
      "train loss:0.6950984307766379\n",
      "train loss:0.7132882441622135\n",
      "train loss:0.7707786494298083\n",
      "train loss:0.8904055194814552\n",
      "train loss:0.4764778048643534\n",
      "train loss:0.6858142246771723\n",
      "train loss:0.8569476742747784\n",
      "train loss:0.5260260124606211\n",
      "train loss:0.5472840823634482\n",
      "train loss:0.5441857303188867\n",
      "train loss:0.7054073964913317\n",
      "train loss:0.499628988871638\n",
      "train loss:0.6459975940829\n",
      "train loss:0.5319843440382027\n",
      "train loss:0.6645830041990965\n",
      "train loss:0.39193503252018286\n",
      "train loss:0.633158749513691\n",
      "train loss:0.6672958582911386\n",
      "train loss:0.7113482539386531\n",
      "train loss:0.526345375870092\n",
      "train loss:0.6151843584150121\n",
      "train loss:0.7933851020250088\n",
      "train loss:0.5463083231060282\n",
      "train loss:0.5234291613455644\n",
      "train loss:0.6613456104952422\n",
      "train loss:0.969733196375681\n",
      "train loss:0.3785842596072332\n",
      "train loss:0.6956453044769096\n",
      "train loss:0.3367360050917857\n",
      "train loss:0.8144558674306672\n",
      "train loss:0.5490980846636229\n",
      "train loss:0.6503546239521578\n",
      "train loss:0.6620193285274908\n",
      "train loss:0.4327322397340271\n",
      "train loss:0.6889078591447174\n",
      "train loss:0.3890257930844213\n",
      "train loss:0.5147290510680195\n",
      "train loss:0.6803648654937989\n",
      "train loss:0.8180577453137847\n",
      "train loss:0.44662184403094385\n",
      "train loss:0.5328774849273485\n",
      "train loss:0.7212242143947672\n",
      "train loss:0.8051879352158474\n",
      "train loss:0.9303231975571187\n",
      "train loss:0.5787690260482633\n",
      "train loss:0.5459405893666279\n",
      "train loss:0.33979629517318677\n",
      "train loss:0.5671018518013758\n",
      "train loss:0.5190833625385431\n",
      "train loss:0.5300617833558369\n",
      "train loss:0.3629834599879251\n",
      "train loss:0.5843982468113322\n",
      "train loss:0.7249671019894997\n",
      "train loss:0.8470250833771844\n",
      "train loss:0.5243277724155953\n",
      "train loss:0.6538579499743954\n",
      "train loss:0.7307398854665209\n",
      "train loss:0.7169475349284568\n",
      "train loss:0.3419101369337483\n",
      "train loss:0.6653969877334953\n",
      "train loss:0.845354111774092\n",
      "train loss:0.536116885655265\n",
      "train loss:0.6826843452894877\n",
      "train loss:0.36832977768454084\n",
      "train loss:0.5510644816424173\n",
      "train loss:0.8579232477769191\n",
      "train loss:0.7158216150955999\n",
      "train loss:0.5548626949971054\n",
      "train loss:0.3677416413042571\n",
      "train loss:0.8814230704384507\n",
      "train loss:0.7980559425242685\n",
      "train loss:0.3195031752809279\n",
      "train loss:1.0065163947062894\n",
      "train loss:0.5154603529035421\n",
      "train loss:0.5346634236076202\n",
      "train loss:0.665173665377526\n",
      "train loss:0.755632797971532\n",
      "train loss:0.6609179477024245\n",
      "train loss:0.549768053754106\n",
      "train loss:0.5370881567068585\n",
      "train loss:0.3882984122046339\n",
      "train loss:0.5641751179512784\n",
      "train loss:0.6974044612117984\n",
      "train loss:0.6726860926013568\n",
      "train loss:0.9382687842178317\n",
      "train loss:0.5310667082762948\n",
      "train loss:0.8596546349067034\n",
      "train loss:0.6492171291399843\n",
      "train loss:0.5500207482513145\n",
      "train loss:0.5432861817154416\n",
      "train loss:0.6740656957270545\n",
      "train loss:0.6663332501134471\n",
      "train loss:0.5485938991052955\n",
      "train loss:0.5610815571941642\n",
      "train loss:0.5472359709179375\n",
      "train loss:0.5137812646954573\n",
      "train loss:0.8692704177782872\n",
      "train loss:0.4777878250076033\n",
      "train loss:0.6741142111755779\n",
      "train loss:0.3270196586798364\n",
      "train loss:0.39210141193690395\n",
      "train loss:0.5188607602362507\n",
      "train loss:0.5608238133668625\n",
      "train loss:0.6953624499393526\n",
      "train loss:0.5182772678151977\n",
      "train loss:0.5356190208506588\n",
      "train loss:0.518380715884877\n",
      "train loss:0.7195512654670424\n",
      "train loss:0.4960648361128501\n",
      "train loss:0.6952887046375544\n",
      "train loss:0.36507520032911656\n",
      "train loss:0.519725319421469\n",
      "train loss:0.9232281017255486\n",
      "train loss:0.3023449547447613\n",
      "train loss:0.9414741360121722\n",
      "train loss:0.5005221204965796\n",
      "train loss:1.301399091420484\n",
      "train loss:0.3321183822486169\n",
      "train loss:1.1161807512518953\n",
      "train loss:0.45166238680230625\n",
      "train loss:0.6733734255161712\n",
      "train loss:0.6775621533788436\n",
      "train loss:0.6976408534749122\n",
      "train loss:0.497058294385773\n",
      "train loss:0.6786549293454243\n",
      "train loss:0.6997389794337172\n",
      "train loss:0.5767584724326036\n",
      "train loss:0.7039806006057329\n",
      "train loss:0.8071709658004146\n",
      "train loss:0.6736328426313268\n",
      "train loss:0.6743511320922655\n",
      "train loss:0.9808749531150053\n",
      "train loss:0.5633827860404901\n",
      "train loss:0.4654892105874608\n",
      "train loss:0.5747233584200608\n",
      "train loss:0.46372309806036505\n",
      "train loss:0.5670894968101067\n",
      "train loss:0.5731180957910266\n",
      "train loss:0.7968974470999786\n",
      "train loss:0.674744522245537\n",
      "train loss:0.5627903265306875\n",
      "train loss:0.6306351898661056\n",
      "train loss:0.5411103169091422\n",
      "train loss:0.43120895105759394\n",
      "train loss:0.6830988727284836\n",
      "train loss:0.5819382708343388\n",
      "train loss:0.5366078408194543\n",
      "train loss:0.41750033579749746\n",
      "train loss:0.550956185336025\n",
      "train loss:0.6707309968099912\n",
      "train loss:0.7809229206059485\n",
      "train loss:0.5898619760088408\n",
      "train loss:0.72159379578472\n",
      "train loss:0.5362208453020533\n",
      "train loss:0.6707629434638057\n",
      "train loss:0.6916728803363179\n",
      "train loss:0.6957191954814556\n",
      "train loss:0.8034821588423793\n",
      "train loss:0.40066810668974917\n",
      "train loss:0.5443955307580741\n",
      "train loss:0.37186285264491936\n",
      "train loss:0.5284880203399949\n",
      "train loss:0.8469893491701581\n",
      "train loss:0.36063658080966904\n",
      "train loss:0.5564105548776674\n",
      "train loss:0.6869512688816891\n",
      "train loss:0.5362240398813801\n",
      "train loss:0.4953040632546696\n",
      "train loss:0.5345362898176521\n",
      "train loss:0.7172529569635416\n",
      "train loss:0.32278979493150567\n",
      "train loss:0.9211205147701393\n",
      "train loss:0.5741571485898992\n",
      "train loss:0.7019580366287211\n",
      "train loss:0.7590870499407081\n",
      "train loss:0.7058263885437743\n",
      "train loss:0.6775130883674539\n",
      "train loss:0.683940264036815\n",
      "train loss:0.6422652312317532\n",
      "train loss:0.36789008529744033\n",
      "train loss:0.5470743898753091\n",
      "train loss:0.8525874580736852\n",
      "train loss:0.3754354828766034\n",
      "train loss:0.6699591398159467\n",
      "train loss:0.6926410225339341\n",
      "train loss:0.8017203812899047\n",
      "train loss:0.7112239578213455\n",
      "train loss:0.8761400616299166\n",
      "train loss:0.6893095710213766\n",
      "train loss:0.6864326944886827\n",
      "train loss:0.548155687129052\n",
      "train loss:0.8860916212617992\n",
      "train loss:0.7816349647893057\n",
      "train loss:0.4542466766463938\n",
      "train loss:0.4173773766621519\n",
      "train loss:0.7834198381223072\n",
      "train loss:0.6644186541720083\n",
      "train loss:0.4408455773535917\n",
      "train loss:0.4294370229544503\n",
      "train loss:0.5630653803624993\n",
      "train loss:0.5493847624530364\n",
      "train loss:0.7005708245713309\n",
      "train loss:0.4288003212050361\n",
      "train loss:0.43387272710226543\n",
      "train loss:0.5271957340206761\n",
      "train loss:0.8125240958517417\n",
      "train loss:0.5642801721681615\n",
      "train loss:0.7886864233416585\n",
      "train loss:0.4042234550899468\n",
      "train loss:0.5295912120077119\n",
      "train loss:0.5242644546421034\n",
      "train loss:0.381984751585759\n",
      "train loss:0.5279077335076794\n",
      "train loss:0.33409137724764226\n",
      "train loss:0.5064879317830135\n",
      "train loss:0.49530396126392773\n",
      "train loss:0.7033414280833588\n",
      "train loss:0.6966040374279199\n",
      "train loss:0.4732290428454998\n",
      "train loss:0.2919972285691847\n",
      "train loss:0.7001458058704761\n",
      "train loss:0.25225144090260593\n",
      "train loss:0.989648834786179\n",
      "train loss:0.5185647127827223\n",
      "train loss:1.0246601399600102\n",
      "train loss:0.9499993047905642\n",
      "train loss:0.5126955981779329\n",
      "train loss:0.3213024320714385\n",
      "train loss:0.4940859583100069\n",
      "train loss:0.5254472597796847\n",
      "train loss:0.6960002426501493\n",
      "train loss:0.712119503722886\n",
      "train loss:0.29454946731774123\n",
      "train loss:0.48199834444518386\n",
      "train loss:0.725873762424342\n",
      "train loss:0.3125425371609022\n",
      "train loss:0.7328690240047866\n",
      "train loss:0.4808328193257149\n",
      "train loss:0.5160566528062857\n",
      "train loss:0.8954282630952439\n",
      "train loss:0.5027649071860402\n",
      "train loss:0.5188117704646895\n",
      "train loss:0.741177196636587\n",
      "train loss:0.532245956707827\n",
      "train loss:0.6847057473865419\n",
      "train loss:0.5170988899135477\n",
      "train loss:0.5127552646335867\n",
      "train loss:1.0099683891015725\n",
      "train loss:0.5347319045313743\n",
      "train loss:0.7156746059273317\n",
      "train loss:0.6519519742819997\n",
      "train loss:0.525960476394773\n",
      "train loss:0.7048035941233157\n",
      "train loss:0.505802013907314\n",
      "train loss:0.547415561395756\n",
      "train loss:0.7141901782509207\n",
      "train loss:0.8659024882394786\n",
      "train loss:0.3798661276964882\n",
      "train loss:0.8423368390514598\n",
      "train loss:0.3512570146758997\n",
      "train loss:0.5395696907196651\n",
      "train loss:0.5269437848509503\n",
      "train loss:0.3591196634707875\n",
      "train loss:0.3816182705507361\n",
      "train loss:0.6529226555342522\n",
      "train loss:0.8601814204530944\n",
      "train loss:0.517348639889408\n",
      "train loss:0.3181925052421391\n",
      "train loss:0.9018090767290092\n",
      "train loss:0.7201058170699405\n",
      "train loss:0.6943887937023308\n",
      "train loss:0.5018296193147527\n",
      "train loss:0.9022943481714464\n",
      "train loss:0.47570841565440397\n",
      "train loss:0.4662032676785303\n",
      "train loss:0.7230641484509566\n",
      "train loss:0.35710431498318684\n",
      "train loss:0.5356111433168137\n",
      "train loss:0.5316019085236395\n",
      "train loss:0.7057117848247298\n",
      "train loss:0.8436047692463922\n",
      "train loss:0.5650329317617222\n",
      "train loss:0.714336434607199\n",
      "train loss:0.3699106110842801\n",
      "train loss:0.8860029252307402\n",
      "train loss:0.35413453418953705\n",
      "train loss:0.6872204643172493\n",
      "train loss:0.827826571270631\n",
      "train loss:0.38338036248392504\n",
      "train loss:0.8732719124504458\n",
      "train loss:0.7044102387603656\n",
      "train loss:0.8318904268680438\n",
      "train loss:0.7058118861527893\n",
      "train loss:0.66326030498239\n",
      "train loss:0.5356270568863579\n",
      "train loss:0.5347671586922063\n",
      "train loss:0.308084891655675\n",
      "train loss:0.6926555051865124\n",
      "train loss:0.6807430213243021\n",
      "train loss:0.82626318015165\n",
      "train loss:0.40280721912869416\n",
      "train loss:0.6921891809789106\n",
      "train loss:0.529492598977437\n",
      "train loss:0.6884986095812382\n",
      "train loss:0.6670738431601274\n",
      "train loss:0.7946501563435339\n",
      "train loss:0.5455838672259421\n",
      "train loss:0.8086049640737791\n",
      "train loss:0.8220913131562998\n",
      "train loss:0.7882918985911029\n",
      "train loss:0.422629165180142\n",
      "train loss:0.713341590684228\n",
      "train loss:0.6690433270553248\n",
      "train loss:0.6624396469589746\n",
      "train loss:0.5615661065190488\n",
      "train loss:0.6803233266147057\n",
      "train loss:0.6786327451600812\n",
      "train loss:0.5542615307730129\n",
      "train loss:0.6849523753769461\n",
      "train loss:0.8134332978105954\n",
      "train loss:0.6810629145850438\n",
      "train loss:0.7798173272248545\n",
      "train loss:0.6731276123924649\n",
      "train loss:0.5665521143116433\n",
      "train loss:0.5649108931794546\n",
      "train loss:0.5658938161410461\n",
      "train loss:0.7884130589866741\n",
      "train loss:0.672992625412709\n",
      "train loss:0.6710595123022125\n",
      "train loss:0.5525041974299217\n",
      "train loss:0.661612951180216\n",
      "train loss:0.6673689824938849\n",
      "train loss:0.462222422120124\n",
      "train loss:0.5625532192320979\n",
      "train loss:0.5741676097919368\n",
      "train loss:0.7882830957041747\n",
      "train loss:0.5948692608387325\n",
      "train loss:0.5667353999988067\n",
      "train loss:0.5718304929288203\n",
      "train loss:0.4277981924061992\n",
      "train loss:0.7941629478334258\n",
      "train loss:0.4332689217642159\n",
      "train loss:0.5578572267776785\n",
      "train loss:0.49470059245340503\n",
      "train loss:0.400374605694464\n",
      "train loss:0.3939074844058303\n",
      "train loss:0.6450744348518179\n",
      "train loss:0.9799260201006588\n",
      "train loss:0.36352185649866964\n",
      "train loss:0.6974778499117059\n",
      "train loss:0.501282047793062\n",
      "train loss:0.5422322361298987\n",
      "train loss:0.6945664009684075\n",
      "train loss:0.6885080593926738\n",
      "train loss:0.3559520671837096\n",
      "train loss:0.716982995469693\n",
      "train loss:0.508486725275083\n",
      "train loss:0.512513205895917\n",
      "train loss:0.900894905998047\n",
      "train loss:0.3232342339909209\n",
      "train loss:0.5144202586353046\n",
      "train loss:0.5315631667803872\n",
      "train loss:0.7038663543065278\n",
      "train loss:0.5254734859560973\n",
      "train loss:0.31316352582125023\n",
      "train loss:0.5154522396898691\n",
      "train loss:0.7427314507256753\n",
      "train loss:0.5141058496932305\n",
      "train loss:0.7595448456751398\n",
      "train loss:0.712719670938694\n",
      "train loss:0.5197422658828001\n",
      "train loss:0.6797216624144866\n",
      "train loss:0.5163667817852432\n",
      "train loss:0.7247043343349111\n",
      "train loss:0.5207634146232163\n",
      "train loss:0.5228163653363533\n",
      "train loss:0.5004575314541623\n",
      "train loss:0.3036247174778239\n",
      "train loss:0.9171911302050464\n",
      "train loss:0.9234078143717783\n",
      "train loss:0.30468348521074395\n",
      "train loss:0.6913227980457901\n",
      "train loss:0.9040237341308075\n",
      "train loss:0.5677182614533274\n",
      "train loss:0.47374177272154877\n",
      "train loss:0.743464992495414\n",
      "train loss:0.8504289025850202\n",
      "train loss:0.5179415015855876\n",
      "train loss:0.8485170569261425\n",
      "train loss:0.8997065875770345\n",
      "train loss:0.5384584665987646\n",
      "train loss:0.39457436282977554\n",
      "train loss:0.41643475565421434\n",
      "train loss:0.5490566053011323\n",
      "train loss:0.5039964082875384\n",
      "train loss:0.6413031233015746\n",
      "train loss:0.5458770237099186\n",
      "train loss:0.5540323511374516\n",
      "train loss:0.3675932982168176\n",
      "train loss:0.7031628542484446\n",
      "train loss:0.5409002384914026\n",
      "train loss:0.7137501871254004\n",
      "train loss:0.7159826221844682\n",
      "train loss:0.755835883645557\n",
      "train loss:0.381527059440441\n",
      "train loss:0.7208086491146564\n",
      "train loss:0.6741540079970634\n",
      "train loss:0.5102856085503655\n",
      "train loss:0.5247377526004906\n",
      "train loss:0.5280892327054152\n",
      "train loss:0.5534909257586875\n",
      "train loss:0.6776763232156087\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6895424836601307\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#from dataset.mnist import load_mnist\n",
    "from common.trainer import Trainer\n",
    "\n",
    "normal_dir = os.path.join('.', 'Normal_images')\n",
    "glaucoma_dir = os.path.join('.','Glaucoma_images')\n",
    "\n",
    "normal_files = glob.glob(normal_dir+'/*.jpg')\n",
    "glaucoma_files = glob.glob(glaucoma_dir+'/*.jpg')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "files_df = pd.DataFrame({\n",
    "    'filename': normal_files + glaucoma_files,\n",
    "    'label': [1] * len(glaucoma_files) + [0] * len(normal_files)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "train_files, test_files, train_labels, test_labels =  train_test_split(files_df['filename'].values,\n",
    "                                                                       files_df['label'].values,\n",
    "                                                                       test_size=0.3, random_state=42)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(train_files,\n",
    "                                                                    train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "#print('Train:', Counter(train_labels), '\\nVal:', Counter(val_labels), '\\nTest:', Counter(test_labels))\n",
    "\n",
    "import cv2\n",
    "\n",
    "def make_data(filenames):\n",
    "    data_map = []\n",
    "    for imgfname in filenames:\n",
    "        img = cv2.imread(imgfname)\n",
    "        img = cv2.resize(img, (64,64), interpolation=cv2.INTER_AREA)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.expand_dims(img, axis = -1)\n",
    "        data_map.append(img)        \n",
    "    return np.array(data_map)\n",
    "\n",
    "train_data = make_data(train_files)\n",
    "val_data = make_data(val_files)\n",
    "test_data = make_data(test_files)\n",
    "\n",
    "imgplot = plt.imshow(test_data[10])\n",
    "plt.show()\n",
    "\n",
    "train_data = train_data.transpose(0,3,1,2)\n",
    "test_data = test_data.transpose(0,3,1,2)\n",
    "print(train_data.shape)\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,64,64), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=2, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, train_data, train_labels, test_data, test_labels,\n",
    "                  epochs=max_epochs, mini_batch_size=5,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=None)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "#network.save_params(\"params.pkl\")\n",
    "#print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "#x = np.arange(max_epochs)\n",
    "x = np.arange(len(trainer.train_acc_list))\n",
    "plt.plot(x, trainer.train_acc_list, color='k', marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, color='r', marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.03)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddccc7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
