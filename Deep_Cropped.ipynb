{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12eefeea-a284-4e12-8968-4ce2fad5d97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\oykwon\\anaconda3\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\oykwon\\anaconda3\\lib\\site-packages (from opencv-python) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "## opencv 설치\n",
    "!pip install opencv-python  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6f1769d-a114-4f7f-b826-74cf14217887",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정상인 사진과 녹내장인 사진의 분류\n",
    "## 현재 작업하는 디렉토리 아래에 G1020 이미지가 있다고 가정\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "file_path = './G1020/G1020.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "normal_dir = './Normal_images_cropped/'\n",
    "glaucoma_dir = './Glaucoma_images_cropped/'\n",
    "\n",
    "if not os.path.exists(normal_dir):\n",
    "    os.makedirs(normal_dir)\n",
    "if not os.path.exists(glaucoma_dir):\n",
    "    os.makedirs(glaucoma_dir)\n",
    "\n",
    "i = 0\n",
    "while (i < len(df)):\n",
    "    src = './G1020/Images_Cropped/img/'+df['imageID'][i]\n",
    "    if (df['binaryLabels'][i]) == 0:\n",
    "        des = normal_dir\n",
    "    else:\n",
    "        des = glaucoma_dir\n",
    "    des = des + df['imageID'][i]\n",
    "    shutil.copy(src, des)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e98290-e729-4f55-851a-ff75c1a0c188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "\n",
    "class DeepConvNet:\n",
    "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\n",
    "\n",
    "    네트워크 구성은 아래와 같음\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        conv - relu - conv- relu - pool -\n",
    "        affine - relu - dropout - affine - dropout - softmax\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 64, 64),\n",
    "                 conv_param_1 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_2 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_3 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_4 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_5 = {'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 conv_param_6 = {'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
    "                 hidden_size=50, output_size=2):\n",
    "        # 가중치 초기화===========\n",
    "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\n",
    "        pre_node_nums = np.array([1*3*3, 32*3*3, 32*3*3, 364*3*3, 64*3*3, 128*3*3, 128*8*8, hidden_size])\n",
    "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\n",
    "        \n",
    "        self.params = {}\n",
    "        pre_channel_num = input_dim[0]\n",
    "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
    "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
    "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
    "            pre_channel_num = conv_param['filter_num']\n",
    "        self.params['W7'] = wight_init_scales[6] * np.random.randn(128*8*8, hidden_size)\n",
    "        self.params['b7'] = np.zeros(hidden_size)\n",
    "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b8'] = np.zeros(output_size)\n",
    "\n",
    "        # 계층 생성===========\n",
    "        self.layers = []\n",
    "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
    "                           conv_param_1['stride'], conv_param_1['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
    "                           conv_param_2['stride'], conv_param_2['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
    "                           conv_param_3['stride'], conv_param_3['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
    "                           conv_param_4['stride'], conv_param_4['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
    "                           conv_param_5['stride'], conv_param_5['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
    "                           conv_param_6['stride'], conv_param_6['pad']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
    "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
    "        self.layers.append(Relu())\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
    "        self.layers.append(Dropout(0.5))\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Dropout):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x, train_flg=True)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def accuracy(self, x, t, batch_size=100):\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        acc = 0.0\n",
    "\n",
    "        for i in range(int(x.shape[0] / batch_size)):\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\n",
    "            y = self.predict(tx, train_flg=False)\n",
    "            y = np.argmax(y, axis=1)\n",
    "            acc += np.sum(y == tt)\n",
    "\n",
    "        return acc / x.shape[0]\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        tmp_layers = self.layers.copy()\n",
    "        tmp_layers.reverse()\n",
    "        for layer in tmp_layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
    "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def save_params(self, file_name=\"params.pkl\"):\n",
    "        params = {}\n",
    "        for key, val in self.params.items():\n",
    "            params[key] = val\n",
    "        with open(file_name, 'wb') as f:\n",
    "            pickle.dump(params, f)\n",
    "\n",
    "    def load_params(self, file_name=\"params.pkl\"):\n",
    "        with open(file_name, 'rb') as f:\n",
    "            params = pickle.load(f)\n",
    "        for key, val in params.items():\n",
    "            self.params[key] = val\n",
    "\n",
    "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
    "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
    "            self.layers[layer_idx].b = self.params['b' + str(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f1a6357-14a0-431e-b8fb-ef48beb95035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "normal_dir = os.path.join('.', 'Normal_images_cropped')\n",
    "glaucoma_dir = os.path.join('.','Glaucoma_images_cropped')\n",
    "    \n",
    "normal_files = glob.glob(normal_dir+'/*.jpg')\n",
    "glaucoma_files = glob.glob(glaucoma_dir+'/*.jpg')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "files_df = pd.DataFrame({\n",
    "    'filename': normal_files + glaucoma_files,\n",
    "    'label': [1] * len(glaucoma_files) + [0] * len(normal_files)\n",
    "}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_files, test_files, train_labels, test_labels =  train_test_split(files_df['filename'].values,\n",
    "                                                                       files_df['label'].values,\n",
    "                                                                       test_size=0.3, random_state=42)\n",
    "train_files, val_files, train_labels, val_labels = train_test_split(train_files,\n",
    "                                                                    train_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "#print('Train:', Counter(train_labels), '\\nVal:', Counter(val_labels), '\\nTest:', Counter(test_labels))\n",
    "\n",
    "def make_data(filenames):\n",
    "    data_map = []\n",
    "    for imgfname in filenames:\n",
    "        img = cv2.imread(imgfname)\n",
    "        img = cv2.resize(img, (64,64), interpolation=cv2.INTER_AREA)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = np.expand_dims(img, axis = -1)\n",
    "        data_map.append(img)        \n",
    "    return np.array(data_map)\n",
    "\n",
    "train_data = make_data(train_files)\n",
    "val_data = make_data(val_files)\n",
    "test_data = make_data(test_files)\n",
    "\n",
    "#print(train_data.shape)\n",
    "#imgplot = plt.imshow(test_data[10])\n",
    "#plt.show()\n",
    "\n",
    "train_data = train_data.transpose(0,3,1,2)\n",
    "test_data = test_data.transpose(0,3,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d17cae7b-340f-4ba0-b6b4-4bbf86b62805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:-9.999999505838704e-08\n",
      "=== epoch:1, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:6.863126406847042\n",
      "train loss:0.13862931611199703\n",
      "train loss:3.500877882415652\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:3.500877882415652\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:3.500877860502219\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873268479406\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.23283036012924924\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.138629316112185\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.277258732226579\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.1386343009210858\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.1386410346168715\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999492516028e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:3.223619050191668\n",
      "train loss:0.2772587322239891\n",
      "train loss:3.3622484663036603\n",
      "train loss:0.13862956144147234\n",
      "train loss:0.138629316112019\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.27725873222541164\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479733\n",
      "train loss:2.6510426031990413\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931724315125\n",
      "train loss:3.6295933770174855\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:2, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:3.500877882415653\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611206328\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2904468595575413\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.1386293161120033\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.9107259770518233\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.1386293161122586\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.1386293161119972\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611200074\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.99999949695692e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.6931469805599654\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "=== epoch:3, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.27725969240945303\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.853071570185049e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931670274747\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:4, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931642463333\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.6931469805599654\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611217638\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:5, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.971961310193199e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.998031369270747e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479732\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "=== epoch:6, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "=== epoch:7, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "=== epoch:8, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:9, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479732\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "=== epoch:10, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.6931469805599654\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "=== epoch:11, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "=== epoch:12, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "=== epoch:13, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "=== epoch:14, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.5545175644479732\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:3.362248393716537\n",
      "=== epoch:15, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "=== epoch:16, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479732\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.5545175644479732\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:17, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.953359436423221e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.5545175644479732\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.5545175644479733\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999998768650689e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:18, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13863024632112192\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.997300473999503e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.5545175644479733\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "=== epoch:19, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.5545175644479733\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "=== epoch:20, train acc:0.8791208791208791, test acc:0.9174311926605505 ===\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.5545175644479732\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.27725873222398917\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.4158881483359812\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.41588814833598126\n",
      "train loss:0.27725873222398917\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.2772587322239891\n",
      "train loss:0.13862931611199703\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:-9.999999505838704e-08\n",
      "train loss:0.4158881483359812\n",
      "train loss:-9.999999505838704e-08\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.9174311926605505\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caluculate accuracy (float64) ... \n",
      "0.9174311926605505\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "#from deep_convnet import DeepConvNet\n",
    "from common.trainer import Trainer\n",
    "#(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "(x_train, t_train) = (train_data, train_labels)\n",
    "(x_test, t_test) = (test_data, test_labels)\n",
    "\n",
    "network = DeepConvNet()  \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=20, mini_batch_size=5,\n",
    "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
    "                  evaluate_sample_num_per_epoch=None)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보관\n",
    "#network.save_params(\"deep_convnet_params.pkl\")\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "#x = np.arange(max_epochs)\n",
    "x = np.arange(len(trainer.train_acc_list))\n",
    "\n",
    "plt.plot(x, trainer.train_acc_list, color='k', marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, color='r', marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.03)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "print(\"caluculate accuracy (float64) ... \")\n",
    "print(network.accuracy(x_test, t_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735acc4-9bcc-4364-b448-1940490a285b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
